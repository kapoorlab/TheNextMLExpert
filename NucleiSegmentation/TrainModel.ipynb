{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "TrainModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYR0BPbj3RtP"
      },
      "source": [
        "# In this notebook we use Pre-trained model which is specified as copy model and we train it on our own annotated images. This strategy helps when your own training data is limited "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5oeUOvS3RtU",
        "outputId": "a1cf5977-3a90-4d93-8e26-e10e98c7d733"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slUcZTMQ3RtU",
        "outputId": "d7fd9eea-68ad-4555-c852-a4a5fa8bb91e"
      },
      "source": [
        "!pip install tiffile\n",
        "!pip install elasticdeform\n",
        "!pip install keras==2.2.5\n",
        "!pip install csbdeep\n",
        "!pip install stardist\n",
        "!pip install gputools\n",
        "!pip install btrack"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tiffile\n",
            "  Downloading https://files.pythonhosted.org/packages/86/d7/d8fdfc8da77fde224e7f21d0c6612614852242b9631e31ca3366edb0d3f2/tiffile-2018.10.18-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.6/dist-packages (from tiffile) (2020.9.3)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.6/dist-packages (from tifffile->tiffile) (1.19.5)\n",
            "Installing collected packages: tiffile\n",
            "Successfully installed tiffile-2018.10.18\n",
            "Collecting elasticdeform\n",
            "  Downloading https://files.pythonhosted.org/packages/22/6b/fd4693892a2035326c79363f05b6380e46d2f70d11e94d3e1f667c797084/elasticdeform-0.4.7.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from elasticdeform) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from elasticdeform) (1.4.1)\n",
            "Building wheels for collected packages: elasticdeform\n",
            "  Building wheel for elasticdeform (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elasticdeform: filename=elasticdeform-0.4.7-cp36-cp36m-linux_x86_64.whl size=72524 sha256=2847824e246582cc77851a26465979f05abfb2744e4ee741d8ce520a5083d435\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/5e/8a/890fbf14dc7f26d5da56968248eb7b85fd7e72870462e2c3e3\n",
            "Successfully built elasticdeform\n",
            "Installing collected packages: elasticdeform\n",
            "Successfully installed elasticdeform-0.4.7\n",
            "Collecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 14.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.6 (from keras==2.2.5) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.5) (1.19.5)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.4.3\n",
            "Collecting csbdeep\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/4b/f0c9c85114c7660309903ec987128d31c6aef8e3d3ab35a8d30d1c947d5d/csbdeep-0.6.1-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from csbdeep) (3.2.2)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.6/dist-packages (from csbdeep) (2020.9.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from csbdeep) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from csbdeep) (1.4.1)\n",
            "Requirement already satisfied: h5py<3 in /usr/local/lib/python3.6/dist-packages (from csbdeep) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from csbdeep) (1.15.0)\n",
            "Collecting keras<2.4,>=2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from csbdeep) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras<2.4,>=2.1.2->csbdeep) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<2.4,>=2.1.2->csbdeep) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras<2.4,>=2.1.2->csbdeep) (1.0.8)\n",
            "Installing collected packages: keras, csbdeep\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed csbdeep-0.6.1 keras-2.3.1\n",
            "Collecting stardist\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/79/04cf6656a7270b6a46055f7feee38acf62fc5d3384f9bfef7bb6e4d35783/stardist-0.6.1.tar.gz (410kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 12.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: csbdeep>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from stardist) (0.6.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from stardist) (0.48.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from stardist) (0.16.2)\n",
            "Requirement already satisfied: keras<2.4,>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (2.3.1)\n",
            "Requirement already satisfied: h5py<3 in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (2.10.0)\n",
            "Requirement already satisfied: tifffile in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (2020.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from csbdeep>=0.6.0->stardist) (3.2.2)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->stardist) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->stardist) (51.3.3)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->stardist) (7.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->stardist) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->stardist) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->stardist) (2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<2.4,>=2.1.2->csbdeep>=0.6.0->stardist) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras<2.4,>=2.1.2->csbdeep>=0.6.0->stardist) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras<2.4,>=2.1.2->csbdeep>=0.6.0->stardist) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep>=0.6.0->stardist) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep>=0.6.0->stardist) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep>=0.6.0->stardist) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->csbdeep>=0.6.0->stardist) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->stardist) (4.4.2)\n",
            "Building wheels for collected packages: stardist\n",
            "  Building wheel for stardist (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stardist: filename=stardist-0.6.1-cp36-cp36m-linux_x86_64.whl size=1859809 sha256=de10e030bb0617e4c1adab4695b39e870886ecf76711b5887e261ba9aec2c839\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/55/87/80ee468f9edba45cc39668e66e822f1af40e6a68516eb38b51\n",
            "Successfully built stardist\n",
            "Installing collected packages: stardist\n",
            "Successfully installed stardist-0.6.1\n",
            "Collecting gputools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/55/c6b9b63bd97bf09ccc42e01cb1b4e455378ac947a2b1e383d9c64eb15faa/gputools-0.2.9-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 13.0MB/s \n",
            "\u001b[?25hCollecting configparser\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gputools) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from gputools) (1.19.5)\n",
            "Collecting reikna>=0.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/05/e8643dd1efc302291692286fc4fc8cefe277eb7de8a3d95a0e48e7dba2ef/reikna-0.7.5.tar.gz (189kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 18.6MB/s \n",
            "\u001b[?25hCollecting pyopencl>=2016.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/12/7d4171ecfaf61bafdc4a628263d086b8e75ff89f4ada5458ff1fd16d953c/pyopencl-2020.3.1-cp36-cp36m-manylinux1_x86_64.whl (738kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 17.9MB/s \n",
            "\u001b[?25hCollecting scikit-tensor-py3; python_version >= \"3.0\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/91/28aa13c7c056c37d53677881cdff1c7e8da097aea5c0e4cb4faafb44ce4f/scikit_tensor_py3-0.4.1-py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hCollecting mako>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/db/2d2d88b924aa4674a080aae83b59ea19d593250bfe5ed789947c21736785/Mako-1.1.4.tar.gz (479kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 31.4MB/s \n",
            "\u001b[?25hCollecting funcsigs>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pyopencl>=2016.1->gputools) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting pytools>=2017.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/8a/149fa553998155f0a90d92cdbbc60f6c998cc9ad5fe0441b2b82628d001a/pytools-2021.1.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako>=0.8.0->reikna>=0.6.7->gputools) (1.1.1)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2017.6->pyopencl>=2016.1->gputools) (0.8)\n",
            "Building wheels for collected packages: reikna, mako, pytools\n",
            "  Building wheel for reikna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for reikna: filename=reikna-0.7.5-cp36-none-any.whl size=122267 sha256=463c6185bd7e715f560de182a1886084151868baea8420177b9025a957c39f99\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/2d/ba/12c9ba3637183463c471bcf352f5bc1703ab7dfbec9842f04a\n",
            "  Building wheel for mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75675 sha256=913b39e92529df901da3eaa5173a51daf1560485c04af5177dbb4e152f4421be\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/10/d3/aeb26e20d19045e2a68e5d3cbb57432e11b5d9c92c99f98d47\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2021.1-py2.py3-none-any.whl size=60415 sha256=857be1a878a8bd8c8106a4ca3265d88744bf4af8cf11ff54dcbdab4e75955c28\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/85/10/7da05d77b47f2cb503e5c4185ba82187d0b6b6736b3c6641e0\n",
            "Successfully built reikna mako pytools\n",
            "\u001b[31mERROR: scikit-tensor-py3 0.4.1 has requirement numpy==1.16.*, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-tensor-py3 0.4.1 has requirement scipy==1.3.*, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: configparser, mako, funcsigs, reikna, appdirs, pytools, pyopencl, scikit-tensor-py3, gputools\n",
            "Successfully installed appdirs-1.4.4 configparser-5.0.1 funcsigs-1.0.2 gputools-0.2.9 mako-1.1.4 pyopencl-2020.3.1 pytools-2021.1 reikna-0.7.5 scikit-tensor-py3-0.4.1\n",
            "Collecting btrack\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a9/9dabc5937dc5b096b00369ef3517c8f92592c92e490da85c8e653db42dc8/btrack-0.4.0-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from btrack) (3.2.2)\n",
            "Requirement already satisfied: cvxopt>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from btrack) (1.2.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from btrack) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.6/dist-packages (from btrack) (1.19.5)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from btrack) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->btrack) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->btrack) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->btrack) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.1->btrack) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.10.0->btrack) (1.15.0)\n",
            "Installing collected packages: btrack\n",
            "Successfully installed btrack-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdSLVOKB3RtV",
        "outputId": "9440fd11-c64b-4a26-f746-fbf5d2047df2"
      },
      "source": [
        "%cd '/content/drive/My Drive/SmartSeeds/'\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "import glob\n",
        "from Utils import SmartSeeds2D\n",
        "from tifffile import imread, imwrite\n",
        "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SmartSeeds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5BEu4KY3RtV"
      },
      "source": [
        "# In the cell below specify the following:\n",
        "\n",
        "1) Directory where the training data is, inside this directory there should be the two subfolders called Raw and Mask. Inside the Raw folder are the raw images and inside the Mask folder are the labelled images.\n",
        "\n",
        "2) The training data for doing UNET training is stored in NPZ format so please specify the NPZ filename which is suitable for your data.\n",
        "\n",
        "3) Model directory is where the trained Neural network models are stored, please chooose a location if you want to change the default location which is where the training data is.\n",
        "\n",
        "4) Copy Model name is optional, in case you have a previouis trained model and want to re-train it on new data but store it with a new name.\n",
        "\n",
        "5) Model name is the unique name of the trained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHxQs6mE3RtV"
      },
      "source": [
        "Data_dir = '/content/drive/My Drive/FMIChallenge/'\n",
        "NPZ_filename = 'FMINuclei'\n",
        "Model_dir = Data_dir\n",
        "Model_Name = 'FMINuclei'\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDBTWsFO3RtV"
      },
      "source": [
        "# In this cell choose the network training parameters for the Neural Network\n",
        "\n",
        "1) NetworkDepth = Depth of the network, with each increasing depth the image is downsampled by 2 hence the XY dimension of the data / 2^depth has to be greater than 1.\n",
        "\n",
        "2) Epochs, training for longer epochs ensures a well converged network and requires longer GPU runtimes.\n",
        "\n",
        "3) Learning rate is the parameter which controls the step size used in the optimization process and it should not be greater than 0.001 at the start of the training.\n",
        "\n",
        "4) batch size controls the number of images used for doing stochastic gradient descent and is a parameter that is limited by the GPU RAM available, if you do not have a lot of ran batch size < 10 should be optimal. \n",
        "\n",
        "5) PatchX,Y is the patch size used for making patches out of the iamge data. The original image is broken down into patches for training. Patch size is chosen based on having enough context for the network to learn but at the same time not being too big to obscure memory usage.\n",
        "\n",
        "6) Kernel is the receptive field of the neural network, usual choices are 3,5 or 7 but not larger than that. This is the size of the convolutional kernel used in the network\n",
        "\n",
        "7) n_patches_per_image is the number of patches sampled for each image to create the npz file, choose an optimal value so that the file is not too big for the computer memory. \n",
        "\n",
        "8) Rays is the number of rays used the learn the distance map, low rays decreases the spatial resoultion and high rays are able to resolve the shape better.\n",
        "\n",
        "\n",
        "9) OpenCL is a boolean parameter that is set true if you want to do some opencl computations on the GPU, this requires GPU tools but if you do not have them set this to false.\n",
        "\n",
        "Some optimal values have been chosen by default and should work well for any NVDIA enabled GPU computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kueVXEV_3RtW"
      },
      "source": [
        "#Network training parameters\n",
        "NetworkDepth =4\n",
        "Epochs = 200\n",
        "LearningRate = 1.0E-4\n",
        "batch_size = 1\n",
        "PatchX = 256\n",
        "PatchY = 256\n",
        "Kernel = 3\n",
        "DownsampleFactor = 1\n",
        "n_patches_per_image = 16\n",
        "Rays = 128\n",
        "use_gpu_opencl = False\n",
        "GenerateNPZ = False\n",
        "TrainUNET = True\n",
        "TrainSTAR = True\n",
        "grid = (2,2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WR_cz_s3RtW",
        "outputId": "bd383cc0-782e-4f8b-e0b8-d2d902cf27a6"
      },
      "source": [
        "\n",
        "model = SmartSeeds2D(BaseDir = Data_dir, NPZfilename = NPZ_filename, model_name = Model_Name, model_dir = Model_dir, n_patches_per_image = n_patches_per_image, DownsampleFactor = DownsampleFactor, GenerateNPZ = GenerateNPZ,TrainUNET = TrainUNET, TrainSTAR = TrainSTAR, grid = grid, PatchX= PatchX, PatchY= PatchY,  use_gpu = use_gpu_opencl,  batch_size = batch_size, depth = NetworkDepth, kern_size = Kernel, n_rays = Rays, epochs = Epochs, learning_rate = LearningRate)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance segmentation masks: 12\n",
            "Semantic segmentation masks: 12\n",
            "Training StarDistModel model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:00<00:00, 140.67it/s]\n",
            "100%|██████████| 12/12 [00:00<00:00, 199.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12\n",
            "number of images:  12\n",
            "- training:        10\n",
            "- validation:       2\n",
            "Configuration for a :class:`StarDist2D` model.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    axes : str or None\n",
            "        Axes of the input images.\n",
            "    n_rays : int\n",
            "        Number of radial directions for the star-convex polygon.\n",
            "        Recommended to use a power of 2 (default: 32).\n",
            "    n_channel_in : int\n",
            "        Number of channels of given input image (default: 1).\n",
            "    grid : (int,int)\n",
            "        Subsampling factors (must be powers of 2) for each of the axes.\n",
            "        Model will predict on a subsampled grid for increased efficiency and larger field of view.\n",
            "    backbone : str\n",
            "        Name of the neural network architecture to be used as backbone.\n",
            "    kwargs : dict\n",
            "        Overwrite (or add) configuration attributes (see below).\n",
            "\n",
            "\n",
            "    Attributes\n",
            "    ----------\n",
            "    unet_n_depth : int\n",
            "        Number of U-Net resolution levels (down/up-sampling layers).\n",
            "    unet_kernel_size : (int,int)\n",
            "        Convolution kernel size for all (U-Net) convolution layers.\n",
            "    unet_n_filter_base : int\n",
            "        Number of convolution kernels (feature channels) for first U-Net layer.\n",
            "        Doubled after each down-sampling layer.\n",
            "    unet_pool : (int,int)\n",
            "        Maxpooling size for all (U-Net) convolution layers.\n",
            "    net_conv_after_unet : int\n",
            "        Number of filters of the extra convolution layer after U-Net (0 to disable).\n",
            "    unet_* : *\n",
            "        Additional parameters for U-net backbone.\n",
            "    train_shape_completion : bool\n",
            "        Train model to predict complete shapes for partially visible objects at image boundary.\n",
            "    train_completion_crop : int\n",
            "        If 'train_shape_completion' is set to True, specify number of pixels to crop at boundary of training patches.\n",
            "        Should be chosen based on (largest) object sizes.\n",
            "    train_patch_size : (int,int)\n",
            "        Size of patches to be cropped from provided training images.\n",
            "    train_background_reg : float\n",
            "        Regularizer to encourage distance predictions on background regions to be 0.\n",
            "    train_foreground_only : float\n",
            "        Fraction (0..1) of patches that will only be sampled from regions that contain foreground pixels.\n",
            "    train_dist_loss : str\n",
            "        Training loss for star-convex polygon distances ('mse' or 'mae').\n",
            "    train_loss_weights : tuple of float\n",
            "        Weights for losses relating to (probability, distance)\n",
            "    train_epochs : int\n",
            "        Number of training epochs.\n",
            "    train_steps_per_epoch : int\n",
            "        Number of parameter update steps per epoch.\n",
            "    train_learning_rate : float\n",
            "        Learning rate for training.\n",
            "    train_batch_size : int\n",
            "        Batch size for training.\n",
            "    train_n_val_patches : int\n",
            "        Number of patches to be extracted from validation images (``None`` = one patch per image).\n",
            "    train_tensorboard : bool\n",
            "        Enable TensorBoard for monitoring training progress.\n",
            "    train_reduce_lr : dict\n",
            "        Parameter :class:`dict` of ReduceLROnPlateau_ callback; set to ``None`` to disable.\n",
            "    use_gpu : bool\n",
            "        Indicate that the data generator should use OpenCL to do computations on the GPU.\n",
            "\n",
            "        .. _ReduceLROnPlateau: https://keras.io/callbacks/#reducelronplateau\n",
            "    \n",
            "Config2D(axes='YXC', backbone='unet', grid=(2, 2), n_channel_in=1, n_channel_out=129, n_dim=2, n_rays=128, net_conv_after_unet=128, net_input_shape=(None, None, 1), net_mask_shape=(None, None, 1), train_background_reg=0.0001, train_batch_size=4, train_checkpoint='/content/drive/My Drive/FMIChallenge/FMINuclei.h5', train_checkpoint_epoch='weights_now.h5', train_checkpoint_last='weights_last.h5', train_completion_crop=32, train_dist_loss='mae', train_epochs=200, train_foreground_only=0.9, train_learning_rate=0.0001, train_loss_weights=(1, 0.05), train_n_val_patches=None, train_patch_size=(256, 256), train_reduce_lr={'factor': 0.5, 'patience': 40, 'min_delta': 0}, train_shape_completion=False, train_steps_per_epoch=100, train_tensorboard=True, unet_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_kernel_size=(3, 3), unet_last_activation='relu', unet_n_conv_per_depth=2, unet_n_depth=4, unet_n_filter_base=32, unet_pool=(2, 2), unet_prefix='', use_gpu=False)\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Using default values: prob_thresh=0.5, nms_thresh=0.4.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/csbdeep/utils/tf.py:309: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/csbdeep/utils/tf.py:337: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/csbdeep/utils/tf.py:344: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/200\n",
            "  4/100 [>.............................] - ETA: 4:42 - loss: 2.0173 - prob_loss: 0.6916 - dist_loss: 26.5134 - prob_kld: 0.4026 - dist_relevant_mae: 26.5134 - dist_relevant_mse: 952.2837"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.169132). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 184s 2s/step - loss: 1.8643 - prob_loss: 0.5374 - dist_loss: 26.5384 - prob_kld: 0.2474 - dist_relevant_mae: 26.5384 - dist_relevant_mse: 955.0552 - val_loss: 1.7480 - val_prob_loss: 0.4169 - val_dist_loss: 26.6231 - val_prob_kld: 0.1194 - val_dist_relevant_mae: 26.6231 - val_dist_relevant_mse: 949.5963\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/csbdeep/utils/tf.py:372: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 1.5162 - prob_loss: 0.4052 - dist_loss: 22.2202 - prob_kld: 0.1162 - dist_relevant_mae: 22.2196 - dist_relevant_mse: 747.0557 - val_loss: 1.1015 - val_prob_loss: 0.3935 - val_dist_loss: 14.1608 - val_prob_kld: 0.0961 - val_dist_relevant_mae: 14.1589 - val_dist_relevant_mse: 337.0710\n",
            "Epoch 3/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.9992 - prob_loss: 0.3718 - dist_loss: 12.5483 - prob_kld: 0.0813 - dist_relevant_mae: 12.5464 - dist_relevant_mse: 280.7272 - val_loss: 0.9446 - val_prob_loss: 0.3536 - val_dist_loss: 11.8187 - val_prob_kld: 0.0562 - val_dist_relevant_mae: 11.8170 - val_dist_relevant_mse: 249.5852\n",
            "Epoch 4/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.9250 - prob_loss: 0.3556 - dist_loss: 11.3882 - prob_kld: 0.0649 - dist_relevant_mae: 11.3865 - dist_relevant_mse: 238.5555 - val_loss: 0.8976 - val_prob_loss: 0.3413 - val_dist_loss: 11.1246 - val_prob_kld: 0.0439 - val_dist_relevant_mae: 11.1233 - val_dist_relevant_mse: 236.1280\n",
            "Epoch 5/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.8849 - prob_loss: 0.3416 - dist_loss: 10.8655 - prob_kld: 0.0513 - dist_relevant_mae: 10.8642 - dist_relevant_mse: 227.9598 - val_loss: 0.8646 - val_prob_loss: 0.3368 - val_dist_loss: 10.5570 - val_prob_kld: 0.0394 - val_dist_relevant_mae: 10.5559 - val_dist_relevant_mse: 218.3323\n",
            "Epoch 6/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.8304 - prob_loss: 0.3322 - dist_loss: 9.9657 - prob_kld: 0.0417 - dist_relevant_mae: 9.9645 - dist_relevant_mse: 207.1212 - val_loss: 0.7786 - val_prob_loss: 0.3323 - val_dist_loss: 8.9257 - val_prob_kld: 0.0348 - val_dist_relevant_mae: 8.9246 - val_dist_relevant_mse: 167.4446\n",
            "Epoch 7/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.7154 - prob_loss: 0.3319 - dist_loss: 7.6690 - prob_kld: 0.0423 - dist_relevant_mae: 7.6678 - dist_relevant_mse: 136.6438 - val_loss: 0.6842 - val_prob_loss: 0.3542 - val_dist_loss: 6.6007 - val_prob_kld: 0.0568 - val_dist_relevant_mae: 6.5995 - val_dist_relevant_mse: 102.3545\n",
            "Epoch 8/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.6503 - prob_loss: 0.3333 - dist_loss: 6.3406 - prob_kld: 0.0430 - dist_relevant_mae: 6.3393 - dist_relevant_mse: 95.3440 - val_loss: 0.6283 - val_prob_loss: 0.3298 - val_dist_loss: 5.9703 - val_prob_kld: 0.0324 - val_dist_relevant_mae: 5.9690 - val_dist_relevant_mse: 83.1176\n",
            "Epoch 9/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.6152 - prob_loss: 0.3212 - dist_loss: 5.8804 - prob_kld: 0.0314 - dist_relevant_mae: 5.8791 - dist_relevant_mse: 82.4459 - val_loss: 0.6215 - val_prob_loss: 0.3280 - val_dist_loss: 5.8696 - val_prob_kld: 0.0306 - val_dist_relevant_mae: 5.8682 - val_dist_relevant_mse: 72.3009\n",
            "Epoch 10/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.6073 - prob_loss: 0.3215 - dist_loss: 5.7163 - prob_kld: 0.0311 - dist_relevant_mae: 5.7150 - dist_relevant_mse: 79.6170 - val_loss: 0.6139 - val_prob_loss: 0.3262 - val_dist_loss: 5.7537 - val_prob_kld: 0.0288 - val_dist_relevant_mae: 5.7524 - val_dist_relevant_mse: 79.5460\n",
            "Epoch 11/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5912 - prob_loss: 0.3152 - dist_loss: 5.5214 - prob_kld: 0.0255 - dist_relevant_mae: 5.5201 - dist_relevant_mse: 74.5459 - val_loss: 0.6027 - val_prob_loss: 0.3251 - val_dist_loss: 5.5530 - val_prob_kld: 0.0277 - val_dist_relevant_mae: 5.5516 - val_dist_relevant_mse: 71.4283\n",
            "Epoch 12/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.5819 - prob_loss: 0.3131 - dist_loss: 5.3765 - prob_kld: 0.0234 - dist_relevant_mae: 5.3752 - dist_relevant_mse: 71.2921 - val_loss: 0.6030 - val_prob_loss: 0.3258 - val_dist_loss: 5.5443 - val_prob_kld: 0.0283 - val_dist_relevant_mae: 5.5429 - val_dist_relevant_mse: 72.9547\n",
            "Epoch 13/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5819 - prob_loss: 0.3129 - dist_loss: 5.3794 - prob_kld: 0.0226 - dist_relevant_mae: 5.3780 - dist_relevant_mse: 71.2420 - val_loss: 0.5864 - val_prob_loss: 0.3210 - val_dist_loss: 5.3061 - val_prob_kld: 0.0236 - val_dist_relevant_mae: 5.3048 - val_dist_relevant_mse: 64.7435\n",
            "Epoch 14/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5710 - prob_loss: 0.3086 - dist_loss: 5.2487 - prob_kld: 0.0201 - dist_relevant_mae: 5.2473 - dist_relevant_mse: 68.1156 - val_loss: 0.5878 - val_prob_loss: 0.3239 - val_dist_loss: 5.2763 - val_prob_kld: 0.0265 - val_dist_relevant_mae: 5.2750 - val_dist_relevant_mse: 65.4152\n",
            "Epoch 15/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5679 - prob_loss: 0.3129 - dist_loss: 5.1013 - prob_kld: 0.0227 - dist_relevant_mae: 5.1000 - dist_relevant_mse: 66.0129 - val_loss: 0.5825 - val_prob_loss: 0.3196 - val_dist_loss: 5.2590 - val_prob_kld: 0.0222 - val_dist_relevant_mae: 5.2576 - val_dist_relevant_mse: 62.2858\n",
            "Epoch 16/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.5604 - prob_loss: 0.3080 - dist_loss: 5.0467 - prob_kld: 0.0175 - dist_relevant_mae: 5.0453 - dist_relevant_mse: 65.3849 - val_loss: 0.5820 - val_prob_loss: 0.3194 - val_dist_loss: 5.2535 - val_prob_kld: 0.0219 - val_dist_relevant_mae: 5.2521 - val_dist_relevant_mse: 59.6245\n",
            "Epoch 17/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5573 - prob_loss: 0.3082 - dist_loss: 4.9816 - prob_kld: 0.0177 - dist_relevant_mae: 4.9803 - dist_relevant_mse: 63.6585 - val_loss: 0.5774 - val_prob_loss: 0.3204 - val_dist_loss: 5.1394 - val_prob_kld: 0.0230 - val_dist_relevant_mae: 5.1381 - val_dist_relevant_mse: 64.3587\n",
            "Epoch 18/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5586 - prob_loss: 0.3093 - dist_loss: 4.9857 - prob_kld: 0.0196 - dist_relevant_mae: 4.9844 - dist_relevant_mse: 63.6536 - val_loss: 0.5752 - val_prob_loss: 0.3261 - val_dist_loss: 4.9808 - val_prob_kld: 0.0287 - val_dist_relevant_mae: 4.9795 - val_dist_relevant_mse: 59.4090\n",
            "Epoch 19/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.5487 - prob_loss: 0.3080 - dist_loss: 4.8135 - prob_kld: 0.0169 - dist_relevant_mae: 4.8122 - dist_relevant_mse: 60.4487 - val_loss: 0.5631 - val_prob_loss: 0.3169 - val_dist_loss: 4.9229 - val_prob_kld: 0.0195 - val_dist_relevant_mae: 4.9216 - val_dist_relevant_mse: 59.2202\n",
            "Epoch 20/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5424 - prob_loss: 0.3034 - dist_loss: 4.7807 - prob_kld: 0.0150 - dist_relevant_mae: 4.7793 - dist_relevant_mse: 60.0660 - val_loss: 0.5585 - val_prob_loss: 0.3163 - val_dist_loss: 4.8439 - val_prob_kld: 0.0189 - val_dist_relevant_mae: 4.8426 - val_dist_relevant_mse: 57.3610\n",
            "Epoch 21/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.5394 - prob_loss: 0.3041 - dist_loss: 4.7051 - prob_kld: 0.0143 - dist_relevant_mae: 4.7038 - dist_relevant_mse: 59.5047 - val_loss: 0.5585 - val_prob_loss: 0.3157 - val_dist_loss: 4.8554 - val_prob_kld: 0.0183 - val_dist_relevant_mae: 4.8541 - val_dist_relevant_mse: 58.6463\n",
            "Epoch 22/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.5389 - prob_loss: 0.3049 - dist_loss: 4.6806 - prob_kld: 0.0158 - dist_relevant_mae: 4.6792 - dist_relevant_mse: 58.2908 - val_loss: 0.5553 - val_prob_loss: 0.3188 - val_dist_loss: 4.7302 - val_prob_kld: 0.0214 - val_dist_relevant_mae: 4.7289 - val_dist_relevant_mse: 53.6694\n",
            "Epoch 23/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.5289 - prob_loss: 0.3033 - dist_loss: 4.5117 - prob_kld: 0.0145 - dist_relevant_mae: 4.5104 - dist_relevant_mse: 55.5098 - val_loss: 0.5509 - val_prob_loss: 0.3167 - val_dist_loss: 4.6845 - val_prob_kld: 0.0193 - val_dist_relevant_mae: 4.6832 - val_dist_relevant_mse: 55.0135\n",
            "Epoch 24/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5245 - prob_loss: 0.3060 - dist_loss: 4.3700 - prob_kld: 0.0162 - dist_relevant_mae: 4.3687 - dist_relevant_mse: 52.9512 - val_loss: 0.5405 - val_prob_loss: 0.3172 - val_dist_loss: 4.4646 - val_prob_kld: 0.0198 - val_dist_relevant_mae: 4.4633 - val_dist_relevant_mse: 49.8099\n",
            "Epoch 25/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5186 - prob_loss: 0.3067 - dist_loss: 4.2370 - prob_kld: 0.0157 - dist_relevant_mae: 4.2357 - dist_relevant_mse: 51.9928 - val_loss: 0.5342 - val_prob_loss: 0.3169 - val_dist_loss: 4.3459 - val_prob_kld: 0.0195 - val_dist_relevant_mae: 4.3445 - val_dist_relevant_mse: 47.4515\n",
            "Epoch 26/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.5093 - prob_loss: 0.3034 - dist_loss: 4.1196 - prob_kld: 0.0138 - dist_relevant_mae: 4.1183 - dist_relevant_mse: 49.2321 - val_loss: 0.5331 - val_prob_loss: 0.3152 - val_dist_loss: 4.3574 - val_prob_kld: 0.0178 - val_dist_relevant_mae: 4.3561 - val_dist_relevant_mse: 49.8401\n",
            "Epoch 27/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.5050 - prob_loss: 0.3066 - dist_loss: 3.9678 - prob_kld: 0.0158 - dist_relevant_mae: 3.9665 - dist_relevant_mse: 46.6463 - val_loss: 0.5237 - val_prob_loss: 0.3145 - val_dist_loss: 4.1844 - val_prob_kld: 0.0171 - val_dist_relevant_mae: 4.1831 - val_dist_relevant_mse: 47.0459\n",
            "Epoch 28/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4929 - prob_loss: 0.3041 - dist_loss: 3.7763 - prob_kld: 0.0148 - dist_relevant_mae: 3.7750 - dist_relevant_mse: 43.8603 - val_loss: 0.5125 - val_prob_loss: 0.3149 - val_dist_loss: 3.9504 - val_prob_kld: 0.0175 - val_dist_relevant_mae: 3.9491 - val_dist_relevant_mse: 42.9280\n",
            "Epoch 29/200\n",
            "100/100 [==============================] - 190s 2s/step - loss: 0.4884 - prob_loss: 0.3093 - dist_loss: 3.5818 - prob_kld: 0.0180 - dist_relevant_mae: 3.5805 - dist_relevant_mse: 39.3910 - val_loss: 0.4933 - val_prob_loss: 0.3131 - val_dist_loss: 3.6031 - val_prob_kld: 0.0157 - val_dist_relevant_mae: 3.6017 - val_dist_relevant_mse: 35.6222\n",
            "Epoch 30/200\n",
            "100/100 [==============================] - 190s 2s/step - loss: 0.4738 - prob_loss: 0.3034 - dist_loss: 3.4088 - prob_kld: 0.0127 - dist_relevant_mae: 3.4074 - dist_relevant_mse: 36.7723 - val_loss: 0.4929 - val_prob_loss: 0.3143 - val_dist_loss: 3.5718 - val_prob_kld: 0.0169 - val_dist_relevant_mae: 3.5704 - val_dist_relevant_mse: 35.5827\n",
            "Epoch 31/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4694 - prob_loss: 0.3039 - dist_loss: 3.3093 - prob_kld: 0.0155 - dist_relevant_mae: 3.3079 - dist_relevant_mse: 34.3074 - val_loss: 0.4871 - val_prob_loss: 0.3128 - val_dist_loss: 3.4861 - val_prob_kld: 0.0154 - val_dist_relevant_mae: 3.4847 - val_dist_relevant_mse: 33.9918\n",
            "Epoch 32/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4654 - prob_loss: 0.3023 - dist_loss: 3.2612 - prob_kld: 0.0135 - dist_relevant_mae: 3.2598 - dist_relevant_mse: 33.4842 - val_loss: 0.4855 - val_prob_loss: 0.3122 - val_dist_loss: 3.4654 - val_prob_kld: 0.0148 - val_dist_relevant_mae: 3.4641 - val_dist_relevant_mse: 34.1525\n",
            "Epoch 33/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4631 - prob_loss: 0.3018 - dist_loss: 3.2246 - prob_kld: 0.0123 - dist_relevant_mae: 3.2233 - dist_relevant_mse: 33.2886 - val_loss: 0.4832 - val_prob_loss: 0.3145 - val_dist_loss: 3.3727 - val_prob_kld: 0.0171 - val_dist_relevant_mae: 3.3713 - val_dist_relevant_mse: 30.6657\n",
            "Epoch 34/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4615 - prob_loss: 0.3033 - dist_loss: 3.1631 - prob_kld: 0.0116 - dist_relevant_mae: 3.1617 - dist_relevant_mse: 31.4168 - val_loss: 0.4785 - val_prob_loss: 0.3128 - val_dist_loss: 3.3144 - val_prob_kld: 0.0154 - val_dist_relevant_mae: 3.3130 - val_dist_relevant_mse: 30.1421\n",
            "Epoch 35/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4565 - prob_loss: 0.3000 - dist_loss: 3.1286 - prob_kld: 0.0103 - dist_relevant_mae: 3.1272 - dist_relevant_mse: 30.9751 - val_loss: 0.4767 - val_prob_loss: 0.3113 - val_dist_loss: 3.3071 - val_prob_kld: 0.0139 - val_dist_relevant_mae: 3.3057 - val_dist_relevant_mse: 30.3600\n",
            "Epoch 36/200\n",
            "100/100 [==============================] - 190s 2s/step - loss: 0.4575 - prob_loss: 0.3009 - dist_loss: 3.1306 - prob_kld: 0.0103 - dist_relevant_mae: 3.1293 - dist_relevant_mse: 30.2191 - val_loss: 0.4734 - val_prob_loss: 0.3111 - val_dist_loss: 3.2467 - val_prob_kld: 0.0136 - val_dist_relevant_mae: 3.2453 - val_dist_relevant_mse: 29.2704\n",
            "Epoch 37/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4593 - prob_loss: 0.3013 - dist_loss: 3.1615 - prob_kld: 0.0110 - dist_relevant_mae: 3.1601 - dist_relevant_mse: 30.0327 - val_loss: 0.4761 - val_prob_loss: 0.3119 - val_dist_loss: 3.2837 - val_prob_kld: 0.0145 - val_dist_relevant_mae: 3.2824 - val_dist_relevant_mse: 30.0974\n",
            "Epoch 38/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4508 - prob_loss: 0.2989 - dist_loss: 3.0374 - prob_kld: 0.0106 - dist_relevant_mae: 3.0361 - dist_relevant_mse: 29.7648 - val_loss: 0.4736 - val_prob_loss: 0.3107 - val_dist_loss: 3.2584 - val_prob_kld: 0.0132 - val_dist_relevant_mae: 3.2570 - val_dist_relevant_mse: 30.4393\n",
            "Epoch 39/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4553 - prob_loss: 0.3034 - dist_loss: 3.0379 - prob_kld: 0.0127 - dist_relevant_mae: 3.0366 - dist_relevant_mse: 29.6931 - val_loss: 0.4712 - val_prob_loss: 0.3113 - val_dist_loss: 3.1966 - val_prob_kld: 0.0139 - val_dist_relevant_mae: 3.1953 - val_dist_relevant_mse: 29.1381\n",
            "Epoch 40/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4534 - prob_loss: 0.3021 - dist_loss: 3.0271 - prob_kld: 0.0129 - dist_relevant_mae: 3.0258 - dist_relevant_mse: 29.6725 - val_loss: 0.4657 - val_prob_loss: 0.3098 - val_dist_loss: 3.1186 - val_prob_kld: 0.0124 - val_dist_relevant_mae: 3.1172 - val_dist_relevant_mse: 28.4372\n",
            "Epoch 41/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4480 - prob_loss: 0.3006 - dist_loss: 2.9486 - prob_kld: 0.0095 - dist_relevant_mae: 2.9473 - dist_relevant_mse: 27.9399 - val_loss: 0.4656 - val_prob_loss: 0.3091 - val_dist_loss: 3.1306 - val_prob_kld: 0.0116 - val_dist_relevant_mae: 3.1292 - val_dist_relevant_mse: 26.7332\n",
            "Epoch 42/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4464 - prob_loss: 0.3006 - dist_loss: 2.9160 - prob_kld: 0.0095 - dist_relevant_mae: 2.9146 - dist_relevant_mse: 27.2975 - val_loss: 0.4695 - val_prob_loss: 0.3101 - val_dist_loss: 3.1895 - val_prob_kld: 0.0126 - val_dist_relevant_mae: 3.1882 - val_dist_relevant_mse: 29.5390\n",
            "Epoch 43/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4461 - prob_loss: 0.3010 - dist_loss: 2.9019 - prob_kld: 0.0112 - dist_relevant_mae: 2.9005 - dist_relevant_mse: 27.9390 - val_loss: 0.4635 - val_prob_loss: 0.3100 - val_dist_loss: 3.0703 - val_prob_kld: 0.0126 - val_dist_relevant_mae: 3.0690 - val_dist_relevant_mse: 27.6238\n",
            "Epoch 44/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4443 - prob_loss: 0.3006 - dist_loss: 2.8741 - prob_kld: 0.0100 - dist_relevant_mae: 2.8728 - dist_relevant_mse: 26.6507 - val_loss: 0.4588 - val_prob_loss: 0.3100 - val_dist_loss: 2.9763 - val_prob_kld: 0.0125 - val_dist_relevant_mae: 2.9749 - val_dist_relevant_mse: 25.9234\n",
            "Epoch 45/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4448 - prob_loss: 0.3020 - dist_loss: 2.8550 - prob_kld: 0.0111 - dist_relevant_mae: 2.8536 - dist_relevant_mse: 27.3956 - val_loss: 0.4582 - val_prob_loss: 0.3105 - val_dist_loss: 2.9532 - val_prob_kld: 0.0131 - val_dist_relevant_mae: 2.9518 - val_dist_relevant_mse: 26.2259\n",
            "Epoch 46/200\n",
            "100/100 [==============================] - 190s 2s/step - loss: 0.4455 - prob_loss: 0.3021 - dist_loss: 2.8672 - prob_kld: 0.0113 - dist_relevant_mae: 2.8659 - dist_relevant_mse: 26.8512 - val_loss: 0.4625 - val_prob_loss: 0.3123 - val_dist_loss: 3.0038 - val_prob_kld: 0.0149 - val_dist_relevant_mae: 3.0025 - val_dist_relevant_mse: 26.7162\n",
            "Epoch 47/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4402 - prob_loss: 0.3012 - dist_loss: 2.7803 - prob_kld: 0.0101 - dist_relevant_mae: 2.7790 - dist_relevant_mse: 26.0585 - val_loss: 0.4558 - val_prob_loss: 0.3093 - val_dist_loss: 2.9301 - val_prob_kld: 0.0119 - val_dist_relevant_mae: 2.9288 - val_dist_relevant_mse: 24.8486\n",
            "Epoch 48/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4449 - prob_loss: 0.2995 - dist_loss: 2.9082 - prob_kld: 0.0098 - dist_relevant_mae: 2.9068 - dist_relevant_mse: 26.7382 - val_loss: 0.4571 - val_prob_loss: 0.3106 - val_dist_loss: 2.9301 - val_prob_kld: 0.0132 - val_dist_relevant_mae: 2.9288 - val_dist_relevant_mse: 26.1092\n",
            "Epoch 49/200\n",
            "100/100 [==============================] - 190s 2s/step - loss: 0.4414 - prob_loss: 0.3001 - dist_loss: 2.8272 - prob_kld: 0.0086 - dist_relevant_mae: 2.8258 - dist_relevant_mse: 26.2132 - val_loss: 0.4558 - val_prob_loss: 0.3092 - val_dist_loss: 2.9320 - val_prob_kld: 0.0117 - val_dist_relevant_mae: 2.9307 - val_dist_relevant_mse: 26.1262\n",
            "Epoch 50/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4326 - prob_loss: 0.2977 - dist_loss: 2.6981 - prob_kld: 0.0084 - dist_relevant_mae: 2.6967 - dist_relevant_mse: 25.0149 - val_loss: 0.4543 - val_prob_loss: 0.3104 - val_dist_loss: 2.8764 - val_prob_kld: 0.0130 - val_dist_relevant_mae: 2.8751 - val_dist_relevant_mse: 23.9477\n",
            "Epoch 51/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4365 - prob_loss: 0.2994 - dist_loss: 2.7428 - prob_kld: 0.0093 - dist_relevant_mae: 2.7414 - dist_relevant_mse: 25.5127 - val_loss: 0.4485 - val_prob_loss: 0.3095 - val_dist_loss: 2.7801 - val_prob_kld: 0.0120 - val_dist_relevant_mae: 2.7788 - val_dist_relevant_mse: 24.0417\n",
            "Epoch 52/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4299 - prob_loss: 0.2985 - dist_loss: 2.6284 - prob_kld: 0.0084 - dist_relevant_mae: 2.6271 - dist_relevant_mse: 24.5405 - val_loss: 0.4481 - val_prob_loss: 0.3102 - val_dist_loss: 2.7567 - val_prob_kld: 0.0128 - val_dist_relevant_mae: 2.7553 - val_dist_relevant_mse: 23.2541\n",
            "Epoch 53/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4267 - prob_loss: 0.2987 - dist_loss: 2.5585 - prob_kld: 0.0094 - dist_relevant_mae: 2.5572 - dist_relevant_mse: 23.1541 - val_loss: 0.4477 - val_prob_loss: 0.3095 - val_dist_loss: 2.7625 - val_prob_kld: 0.0121 - val_dist_relevant_mae: 2.7611 - val_dist_relevant_mse: 24.1138\n",
            "Epoch 54/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4295 - prob_loss: 0.3005 - dist_loss: 2.5794 - prob_kld: 0.0106 - dist_relevant_mae: 2.5780 - dist_relevant_mse: 22.8813 - val_loss: 0.4430 - val_prob_loss: 0.3096 - val_dist_loss: 2.6685 - val_prob_kld: 0.0121 - val_dist_relevant_mae: 2.6672 - val_dist_relevant_mse: 22.7841\n",
            "Epoch 55/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4258 - prob_loss: 0.2999 - dist_loss: 2.5175 - prob_kld: 0.0086 - dist_relevant_mae: 2.5161 - dist_relevant_mse: 22.6358 - val_loss: 0.4414 - val_prob_loss: 0.3088 - val_dist_loss: 2.6517 - val_prob_kld: 0.0114 - val_dist_relevant_mae: 2.6504 - val_dist_relevant_mse: 22.3908\n",
            "Epoch 56/200\n",
            "100/100 [==============================] - 189s 2s/step - loss: 0.4253 - prob_loss: 0.2991 - dist_loss: 2.5237 - prob_kld: 0.0092 - dist_relevant_mae: 2.5224 - dist_relevant_mse: 21.9785 - val_loss: 0.4461 - val_prob_loss: 0.3094 - val_dist_loss: 2.7331 - val_prob_kld: 0.0120 - val_dist_relevant_mae: 2.7317 - val_dist_relevant_mse: 21.6526\n",
            "Epoch 57/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4241 - prob_loss: 0.3000 - dist_loss: 2.4816 - prob_kld: 0.0101 - dist_relevant_mae: 2.4803 - dist_relevant_mse: 21.7634 - val_loss: 0.4398 - val_prob_loss: 0.3110 - val_dist_loss: 2.5745 - val_prob_kld: 0.0136 - val_dist_relevant_mae: 2.5731 - val_dist_relevant_mse: 21.5136\n",
            "Epoch 58/200\n",
            "100/100 [==============================] - 188s 2s/step - loss: 0.4186 - prob_loss: 0.2982 - dist_loss: 2.4075 - prob_kld: 0.0090 - dist_relevant_mae: 2.4061 - dist_relevant_mse: 21.2530 - val_loss: 0.4429 - val_prob_loss: 0.3148 - val_dist_loss: 2.5624 - val_prob_kld: 0.0174 - val_dist_relevant_mae: 2.5610 - val_dist_relevant_mse: 21.4927\n",
            "Epoch 59/200\n",
            "100/100 [==============================] - 187s 2s/step - loss: 0.4179 - prob_loss: 0.2988 - dist_loss: 2.3824 - prob_kld: 0.0087 - dist_relevant_mae: 2.3810 - dist_relevant_mse: 20.9913 - val_loss: 0.4477 - val_prob_loss: 0.3120 - val_dist_loss: 2.7128 - val_prob_kld: 0.0146 - val_dist_relevant_mae: 2.7114 - val_dist_relevant_mse: 20.4294\n",
            "Epoch 60/200\n",
            "100/100 [==============================] - 184s 2s/step - loss: 0.4179 - prob_loss: 0.2994 - dist_loss: 2.3689 - prob_kld: 0.0103 - dist_relevant_mae: 2.3676 - dist_relevant_mse: 19.8388 - val_loss: 0.4384 - val_prob_loss: 0.3092 - val_dist_loss: 2.5833 - val_prob_kld: 0.0118 - val_dist_relevant_mae: 2.5820 - val_dist_relevant_mse: 21.6071\n",
            "Epoch 61/200\n",
            "100/100 [==============================] - 184s 2s/step - loss: 0.4223 - prob_loss: 0.3001 - dist_loss: 2.4437 - prob_kld: 0.0103 - dist_relevant_mae: 2.4423 - dist_relevant_mse: 20.9904 - val_loss: 0.4351 - val_prob_loss: 0.3095 - val_dist_loss: 2.5112 - val_prob_kld: 0.0121 - val_dist_relevant_mae: 2.5099 - val_dist_relevant_mse: 20.7121\n",
            "Epoch 62/200\n",
            "100/100 [==============================] - 182s 2s/step - loss: 0.4157 - prob_loss: 0.2970 - dist_loss: 2.3758 - prob_kld: 0.0087 - dist_relevant_mae: 2.3745 - dist_relevant_mse: 20.0152 - val_loss: 0.4329 - val_prob_loss: 0.3087 - val_dist_loss: 2.4831 - val_prob_kld: 0.0113 - val_dist_relevant_mae: 2.4817 - val_dist_relevant_mse: 19.9126\n",
            "Epoch 63/200\n",
            "100/100 [==============================] - 183s 2s/step - loss: 0.4134 - prob_loss: 0.2983 - dist_loss: 2.3026 - prob_kld: 0.0080 - dist_relevant_mae: 2.3013 - dist_relevant_mse: 19.6217 - val_loss: 0.4315 - val_prob_loss: 0.3090 - val_dist_loss: 2.4518 - val_prob_kld: 0.0115 - val_dist_relevant_mae: 2.4505 - val_dist_relevant_mse: 20.2250\n",
            "Epoch 64/200\n",
            "100/100 [==============================] - 181s 2s/step - loss: 0.4075 - prob_loss: 0.2959 - dist_loss: 2.2310 - prob_kld: 0.0077 - dist_relevant_mae: 2.2296 - dist_relevant_mse: 19.1973 - val_loss: 0.4354 - val_prob_loss: 0.3144 - val_dist_loss: 2.4202 - val_prob_kld: 0.0169 - val_dist_relevant_mae: 2.4189 - val_dist_relevant_mse: 19.7458\n",
            "Epoch 65/200\n",
            "100/100 [==============================] - 182s 2s/step - loss: 0.4059 - prob_loss: 0.2967 - dist_loss: 2.1853 - prob_kld: 0.0079 - dist_relevant_mae: 2.1839 - dist_relevant_mse: 17.9664 - val_loss: 0.4319 - val_prob_loss: 0.3116 - val_dist_loss: 2.4054 - val_prob_kld: 0.0142 - val_dist_relevant_mae: 2.4041 - val_dist_relevant_mse: 19.4993\n",
            "Epoch 66/200\n",
            "  2/100 [..............................] - ETA: 1:32 - loss: 0.3984 - prob_loss: 0.2964 - dist_loss: 2.0412 - prob_kld: 0.0090 - dist_relevant_mae: 2.0399 - dist_relevant_mse: 14.0141"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.244823). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 183s 2s/step - loss: 0.4079 - prob_loss: 0.2978 - dist_loss: 2.2022 - prob_kld: 0.0078 - dist_relevant_mae: 2.2009 - dist_relevant_mse: 18.2894 - val_loss: 0.4363 - val_prob_loss: 0.3082 - val_dist_loss: 2.5617 - val_prob_kld: 0.0108 - val_dist_relevant_mae: 2.5603 - val_dist_relevant_mse: 20.8445\n",
            "Epoch 67/200\n",
            "100/100 [==============================] - 181s 2s/step - loss: 0.4099 - prob_loss: 0.2995 - dist_loss: 2.2076 - prob_kld: 0.0081 - dist_relevant_mae: 2.2063 - dist_relevant_mse: 18.5800 - val_loss: 0.4252 - val_prob_loss: 0.3086 - val_dist_loss: 2.3319 - val_prob_kld: 0.0112 - val_dist_relevant_mae: 2.3305 - val_dist_relevant_mse: 18.2593\n",
            "Epoch 68/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.4081 - prob_loss: 0.2993 - dist_loss: 2.1763 - prob_kld: 0.0091 - dist_relevant_mae: 2.1750 - dist_relevant_mse: 18.4007 - val_loss: 0.4246 - val_prob_loss: 0.3084 - val_dist_loss: 2.3232 - val_prob_kld: 0.0110 - val_dist_relevant_mae: 2.3218 - val_dist_relevant_mse: 18.5047\n",
            "Epoch 69/200\n",
            "100/100 [==============================] - 182s 2s/step - loss: 0.4065 - prob_loss: 0.2970 - dist_loss: 2.1902 - prob_kld: 0.0073 - dist_relevant_mae: 2.1888 - dist_relevant_mse: 18.8256 - val_loss: 0.4267 - val_prob_loss: 0.3087 - val_dist_loss: 2.3604 - val_prob_kld: 0.0112 - val_dist_relevant_mae: 2.3591 - val_dist_relevant_mse: 19.0021\n",
            "Epoch 70/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.4080 - prob_loss: 0.2992 - dist_loss: 2.1767 - prob_kld: 0.0097 - dist_relevant_mae: 2.1753 - dist_relevant_mse: 18.3758 - val_loss: 0.4283 - val_prob_loss: 0.3102 - val_dist_loss: 2.3621 - val_prob_kld: 0.0128 - val_dist_relevant_mae: 2.3607 - val_dist_relevant_mse: 19.1180\n",
            "Epoch 71/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.4059 - prob_loss: 0.2994 - dist_loss: 2.1304 - prob_kld: 0.0092 - dist_relevant_mae: 2.1291 - dist_relevant_mse: 17.9013 - val_loss: 0.4253 - val_prob_loss: 0.3094 - val_dist_loss: 2.3177 - val_prob_kld: 0.0119 - val_dist_relevant_mae: 2.3164 - val_dist_relevant_mse: 17.8562\n",
            "Epoch 72/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.4052 - prob_loss: 0.2980 - dist_loss: 2.1440 - prob_kld: 0.0073 - dist_relevant_mae: 2.1426 - dist_relevant_mse: 17.5414 - val_loss: 0.4238 - val_prob_loss: 0.3070 - val_dist_loss: 2.3364 - val_prob_kld: 0.0096 - val_dist_relevant_mae: 2.3350 - val_dist_relevant_mse: 17.6189\n",
            "Epoch 73/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.4049 - prob_loss: 0.2984 - dist_loss: 2.1302 - prob_kld: 0.0079 - dist_relevant_mae: 2.1288 - dist_relevant_mse: 17.7916 - val_loss: 0.4264 - val_prob_loss: 0.3081 - val_dist_loss: 2.3664 - val_prob_kld: 0.0107 - val_dist_relevant_mae: 2.3651 - val_dist_relevant_mse: 18.7349\n",
            "Epoch 74/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.4032 - prob_loss: 0.2979 - dist_loss: 2.1061 - prob_kld: 0.0069 - dist_relevant_mae: 2.1048 - dist_relevant_mse: 17.3208 - val_loss: 0.4316 - val_prob_loss: 0.3111 - val_dist_loss: 2.4114 - val_prob_kld: 0.0136 - val_dist_relevant_mae: 2.4100 - val_dist_relevant_mse: 17.6650\n",
            "Epoch 75/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.4048 - prob_loss: 0.2965 - dist_loss: 2.1666 - prob_kld: 0.0067 - dist_relevant_mae: 2.1653 - dist_relevant_mse: 17.7763 - val_loss: 0.4253 - val_prob_loss: 0.3081 - val_dist_loss: 2.3448 - val_prob_kld: 0.0107 - val_dist_relevant_mae: 2.3435 - val_dist_relevant_mse: 18.7161\n",
            "Epoch 76/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.4044 - prob_loss: 0.2988 - dist_loss: 2.1131 - prob_kld: 0.0070 - dist_relevant_mae: 2.1118 - dist_relevant_mse: 17.4559 - val_loss: 0.4286 - val_prob_loss: 0.3149 - val_dist_loss: 2.2744 - val_prob_kld: 0.0175 - val_dist_relevant_mae: 2.2730 - val_dist_relevant_mse: 17.8884\n",
            "Epoch 77/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.4030 - prob_loss: 0.2980 - dist_loss: 2.1013 - prob_kld: 0.0088 - dist_relevant_mae: 2.0999 - dist_relevant_mse: 17.3140 - val_loss: 0.4243 - val_prob_loss: 0.3086 - val_dist_loss: 2.3144 - val_prob_kld: 0.0112 - val_dist_relevant_mae: 2.3130 - val_dist_relevant_mse: 18.1797\n",
            "Epoch 78/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.4031 - prob_loss: 0.2982 - dist_loss: 2.0982 - prob_kld: 0.0074 - dist_relevant_mae: 2.0968 - dist_relevant_mse: 17.2266 - val_loss: 0.4249 - val_prob_loss: 0.3074 - val_dist_loss: 2.3486 - val_prob_kld: 0.0100 - val_dist_relevant_mae: 2.3473 - val_dist_relevant_mse: 18.5592\n",
            "Epoch 79/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.4075 - prob_loss: 0.2977 - dist_loss: 2.1967 - prob_kld: 0.0067 - dist_relevant_mae: 2.1953 - dist_relevant_mse: 18.0801 - val_loss: 0.4275 - val_prob_loss: 0.3084 - val_dist_loss: 2.3823 - val_prob_kld: 0.0109 - val_dist_relevant_mae: 2.3809 - val_dist_relevant_mse: 18.9677\n",
            "Epoch 80/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.4018 - prob_loss: 0.2970 - dist_loss: 2.0962 - prob_kld: 0.0073 - dist_relevant_mae: 2.0948 - dist_relevant_mse: 16.8072 - val_loss: 0.4260 - val_prob_loss: 0.3089 - val_dist_loss: 2.3417 - val_prob_kld: 0.0115 - val_dist_relevant_mae: 2.3403 - val_dist_relevant_mse: 18.3901\n",
            "Epoch 81/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.3995 - prob_loss: 0.2966 - dist_loss: 2.0578 - prob_kld: 0.0063 - dist_relevant_mae: 2.0564 - dist_relevant_mse: 17.0802 - val_loss: 0.4187 - val_prob_loss: 0.3072 - val_dist_loss: 2.2291 - val_prob_kld: 0.0098 - val_dist_relevant_mae: 2.2278 - val_dist_relevant_mse: 17.1523\n",
            "Epoch 82/200\n",
            "100/100 [==============================] - 182s 2s/step - loss: 0.3979 - prob_loss: 0.2959 - dist_loss: 2.0410 - prob_kld: 0.0065 - dist_relevant_mae: 2.0396 - dist_relevant_mse: 16.7826 - val_loss: 0.4269 - val_prob_loss: 0.3070 - val_dist_loss: 2.3985 - val_prob_kld: 0.0096 - val_dist_relevant_mae: 2.3971 - val_dist_relevant_mse: 19.1313\n",
            "Epoch 83/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.3987 - prob_loss: 0.2968 - dist_loss: 2.0381 - prob_kld: 0.0063 - dist_relevant_mae: 2.0367 - dist_relevant_mse: 16.6064 - val_loss: 0.4201 - val_prob_loss: 0.3106 - val_dist_loss: 2.1887 - val_prob_kld: 0.0132 - val_dist_relevant_mae: 2.1873 - val_dist_relevant_mse: 16.7222\n",
            "Epoch 84/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.4020 - prob_loss: 0.2974 - dist_loss: 2.0928 - prob_kld: 0.0078 - dist_relevant_mae: 2.0914 - dist_relevant_mse: 16.8395 - val_loss: 0.4172 - val_prob_loss: 0.3068 - val_dist_loss: 2.2081 - val_prob_kld: 0.0093 - val_dist_relevant_mae: 2.2067 - val_dist_relevant_mse: 16.9310\n",
            "Epoch 85/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.3983 - prob_loss: 0.2980 - dist_loss: 2.0058 - prob_kld: 0.0064 - dist_relevant_mae: 2.0044 - dist_relevant_mse: 16.2337 - val_loss: 0.4193 - val_prob_loss: 0.3077 - val_dist_loss: 2.2322 - val_prob_kld: 0.0102 - val_dist_relevant_mae: 2.2308 - val_dist_relevant_mse: 17.3906\n",
            "Epoch 86/200\n",
            "  2/100 [..............................] - ETA: 1:20 - loss: 0.3817 - prob_loss: 0.2884 - dist_loss: 1.8664 - prob_kld: 0.0052 - dist_relevant_mae: 1.8650 - dist_relevant_mse: 12.5612"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.390302). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.195894). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 179s 2s/step - loss: 0.3988 - prob_loss: 0.2981 - dist_loss: 2.0152 - prob_kld: 0.0087 - dist_relevant_mae: 2.0138 - dist_relevant_mse: 15.7964 - val_loss: 0.4183 - val_prob_loss: 0.3067 - val_dist_loss: 2.2332 - val_prob_kld: 0.0092 - val_dist_relevant_mae: 2.2318 - val_dist_relevant_mse: 16.4561\n",
            "Epoch 87/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3963 - prob_loss: 0.2961 - dist_loss: 2.0036 - prob_kld: 0.0065 - dist_relevant_mae: 2.0022 - dist_relevant_mse: 15.9040 - val_loss: 0.4185 - val_prob_loss: 0.3093 - val_dist_loss: 2.1842 - val_prob_kld: 0.0119 - val_dist_relevant_mae: 2.1828 - val_dist_relevant_mse: 16.9391\n",
            "Epoch 88/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3980 - prob_loss: 0.2957 - dist_loss: 2.0446 - prob_kld: 0.0063 - dist_relevant_mae: 2.0432 - dist_relevant_mse: 16.4963 - val_loss: 0.4165 - val_prob_loss: 0.3070 - val_dist_loss: 2.1898 - val_prob_kld: 0.0095 - val_dist_relevant_mae: 2.1884 - val_dist_relevant_mse: 16.8513\n",
            "Epoch 89/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.3996 - prob_loss: 0.2979 - dist_loss: 2.0359 - prob_kld: 0.0067 - dist_relevant_mae: 2.0345 - dist_relevant_mse: 16.7854 - val_loss: 0.4266 - val_prob_loss: 0.3164 - val_dist_loss: 2.2024 - val_prob_kld: 0.0190 - val_dist_relevant_mae: 2.2011 - val_dist_relevant_mse: 17.1343\n",
            "Epoch 90/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3929 - prob_loss: 0.2945 - dist_loss: 1.9683 - prob_kld: 0.0063 - dist_relevant_mae: 1.9669 - dist_relevant_mse: 15.6543 - val_loss: 0.4156 - val_prob_loss: 0.3069 - val_dist_loss: 2.1733 - val_prob_kld: 0.0095 - val_dist_relevant_mae: 2.1719 - val_dist_relevant_mse: 16.8030\n",
            "Epoch 91/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.3950 - prob_loss: 0.2972 - dist_loss: 1.9567 - prob_kld: 0.0065 - dist_relevant_mae: 1.9553 - dist_relevant_mse: 15.7784 - val_loss: 0.4219 - val_prob_loss: 0.3146 - val_dist_loss: 2.1458 - val_prob_kld: 0.0171 - val_dist_relevant_mae: 2.1444 - val_dist_relevant_mse: 16.4531\n",
            "Epoch 92/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3959 - prob_loss: 0.2965 - dist_loss: 1.9879 - prob_kld: 0.0062 - dist_relevant_mae: 1.9865 - dist_relevant_mse: 16.1278 - val_loss: 0.4205 - val_prob_loss: 0.3122 - val_dist_loss: 2.1672 - val_prob_kld: 0.0147 - val_dist_relevant_mae: 2.1658 - val_dist_relevant_mse: 16.3831\n",
            "Epoch 93/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.3970 - prob_loss: 0.2988 - dist_loss: 1.9652 - prob_kld: 0.0071 - dist_relevant_mae: 1.9638 - dist_relevant_mse: 15.9244 - val_loss: 0.4270 - val_prob_loss: 0.3175 - val_dist_loss: 2.1905 - val_prob_kld: 0.0200 - val_dist_relevant_mae: 2.1891 - val_dist_relevant_mse: 16.4916\n",
            "Epoch 94/200\n",
            "100/100 [==============================] - 178s 2s/step - loss: 0.3959 - prob_loss: 0.2977 - dist_loss: 1.9633 - prob_kld: 0.0063 - dist_relevant_mae: 1.9619 - dist_relevant_mse: 15.2971 - val_loss: 0.4156 - val_prob_loss: 0.3066 - val_dist_loss: 2.1788 - val_prob_kld: 0.0092 - val_dist_relevant_mae: 2.1775 - val_dist_relevant_mse: 16.4912\n",
            "Epoch 95/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3998 - prob_loss: 0.2971 - dist_loss: 2.0524 - prob_kld: 0.0074 - dist_relevant_mae: 2.0510 - dist_relevant_mse: 16.5807 - val_loss: 0.4137 - val_prob_loss: 0.3061 - val_dist_loss: 2.1521 - val_prob_kld: 0.0086 - val_dist_relevant_mae: 2.1507 - val_dist_relevant_mse: 16.3080\n",
            "Epoch 96/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.3934 - prob_loss: 0.2963 - dist_loss: 1.9437 - prob_kld: 0.0057 - dist_relevant_mae: 1.9423 - dist_relevant_mse: 15.5864 - val_loss: 0.4139 - val_prob_loss: 0.3079 - val_dist_loss: 2.1210 - val_prob_kld: 0.0104 - val_dist_relevant_mae: 2.1195 - val_dist_relevant_mse: 15.8591\n",
            "Epoch 97/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3936 - prob_loss: 0.2952 - dist_loss: 1.9699 - prob_kld: 0.0060 - dist_relevant_mae: 1.9685 - dist_relevant_mse: 15.8419 - val_loss: 0.4131 - val_prob_loss: 0.3066 - val_dist_loss: 2.1294 - val_prob_kld: 0.0092 - val_dist_relevant_mae: 2.1280 - val_dist_relevant_mse: 16.1752\n",
            "Epoch 98/200\n",
            "100/100 [==============================] - 180s 2s/step - loss: 0.3929 - prob_loss: 0.2958 - dist_loss: 1.9429 - prob_kld: 0.0062 - dist_relevant_mae: 1.9415 - dist_relevant_mse: 15.7955 - val_loss: 0.4137 - val_prob_loss: 0.3062 - val_dist_loss: 2.1508 - val_prob_kld: 0.0088 - val_dist_relevant_mae: 2.1494 - val_dist_relevant_mse: 16.3853\n",
            "Epoch 99/200\n",
            "100/100 [==============================] - 182s 2s/step - loss: 0.3961 - prob_loss: 0.2985 - dist_loss: 1.9533 - prob_kld: 0.0083 - dist_relevant_mae: 1.9519 - dist_relevant_mse: 15.2409 - val_loss: 0.4111 - val_prob_loss: 0.3057 - val_dist_loss: 2.1082 - val_prob_kld: 0.0083 - val_dist_relevant_mae: 2.1068 - val_dist_relevant_mse: 16.0295\n",
            "Epoch 100/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3932 - prob_loss: 0.2982 - dist_loss: 1.8989 - prob_kld: 0.0069 - dist_relevant_mae: 1.8975 - dist_relevant_mse: 15.2691 - val_loss: 0.4095 - val_prob_loss: 0.3067 - val_dist_loss: 2.0554 - val_prob_kld: 0.0093 - val_dist_relevant_mae: 2.0540 - val_dist_relevant_mse: 15.5199\n",
            "Epoch 101/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3917 - prob_loss: 0.2948 - dist_loss: 1.9368 - prob_kld: 0.0059 - dist_relevant_mae: 1.9354 - dist_relevant_mse: 15.3716 - val_loss: 0.4136 - val_prob_loss: 0.3065 - val_dist_loss: 2.1407 - val_prob_kld: 0.0091 - val_dist_relevant_mae: 2.1393 - val_dist_relevant_mse: 16.4639\n",
            "Epoch 102/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3926 - prob_loss: 0.2965 - dist_loss: 1.9232 - prob_kld: 0.0055 - dist_relevant_mae: 1.9218 - dist_relevant_mse: 15.0252 - val_loss: 0.4136 - val_prob_loss: 0.3064 - val_dist_loss: 2.1450 - val_prob_kld: 0.0089 - val_dist_relevant_mae: 2.1436 - val_dist_relevant_mse: 16.4604\n",
            "Epoch 103/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3930 - prob_loss: 0.2975 - dist_loss: 1.9104 - prob_kld: 0.0059 - dist_relevant_mae: 1.9090 - dist_relevant_mse: 14.8555 - val_loss: 0.4128 - val_prob_loss: 0.3086 - val_dist_loss: 2.0852 - val_prob_kld: 0.0111 - val_dist_relevant_mae: 2.0838 - val_dist_relevant_mse: 15.6863\n",
            "Epoch 104/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3914 - prob_loss: 0.2958 - dist_loss: 1.9123 - prob_kld: 0.0065 - dist_relevant_mae: 1.9110 - dist_relevant_mse: 15.1094 - val_loss: 0.4128 - val_prob_loss: 0.3084 - val_dist_loss: 2.0869 - val_prob_kld: 0.0110 - val_dist_relevant_mae: 2.0855 - val_dist_relevant_mse: 15.6225\n",
            "Epoch 105/200\n",
            "  4/100 [>.............................] - ETA: 1:24 - loss: 0.3901 - prob_loss: 0.2897 - dist_loss: 2.0074 - prob_kld: 0.0063 - dist_relevant_mae: 2.0060 - dist_relevant_mse: 18.5217"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.226267). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 169s 2s/step - loss: 0.3887 - prob_loss: 0.2947 - dist_loss: 1.8800 - prob_kld: 0.0065 - dist_relevant_mae: 1.8787 - dist_relevant_mse: 14.8512 - val_loss: 0.4131 - val_prob_loss: 0.3064 - val_dist_loss: 2.1340 - val_prob_kld: 0.0090 - val_dist_relevant_mae: 2.1326 - val_dist_relevant_mse: 16.1857\n",
            "Epoch 106/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3894 - prob_loss: 0.2959 - dist_loss: 1.8709 - prob_kld: 0.0056 - dist_relevant_mae: 1.8695 - dist_relevant_mse: 14.8534 - val_loss: 0.4112 - val_prob_loss: 0.3061 - val_dist_loss: 2.1034 - val_prob_kld: 0.0086 - val_dist_relevant_mae: 2.1020 - val_dist_relevant_mse: 15.8372\n",
            "Epoch 107/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3910 - prob_loss: 0.2961 - dist_loss: 1.8988 - prob_kld: 0.0055 - dist_relevant_mae: 1.8974 - dist_relevant_mse: 15.0120 - val_loss: 0.4134 - val_prob_loss: 0.3075 - val_dist_loss: 2.1164 - val_prob_kld: 0.0101 - val_dist_relevant_mae: 2.1150 - val_dist_relevant_mse: 16.0700\n",
            "Epoch 108/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3896 - prob_loss: 0.2969 - dist_loss: 1.8544 - prob_kld: 0.0068 - dist_relevant_mae: 1.8530 - dist_relevant_mse: 14.3876 - val_loss: 0.4139 - val_prob_loss: 0.3110 - val_dist_loss: 2.0583 - val_prob_kld: 0.0136 - val_dist_relevant_mae: 2.0569 - val_dist_relevant_mse: 15.3295\n",
            "Epoch 109/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3896 - prob_loss: 0.2964 - dist_loss: 1.8638 - prob_kld: 0.0062 - dist_relevant_mae: 1.8624 - dist_relevant_mse: 14.6829 - val_loss: 0.4089 - val_prob_loss: 0.3056 - val_dist_loss: 2.0659 - val_prob_kld: 0.0082 - val_dist_relevant_mae: 2.0645 - val_dist_relevant_mse: 15.5274\n",
            "Epoch 110/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3877 - prob_loss: 0.2943 - dist_loss: 1.8665 - prob_kld: 0.0064 - dist_relevant_mae: 1.8651 - dist_relevant_mse: 14.8185 - val_loss: 0.4072 - val_prob_loss: 0.3064 - val_dist_loss: 2.0160 - val_prob_kld: 0.0090 - val_dist_relevant_mae: 2.0146 - val_dist_relevant_mse: 14.9448\n",
            "Epoch 111/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3892 - prob_loss: 0.2959 - dist_loss: 1.8654 - prob_kld: 0.0059 - dist_relevant_mae: 1.8640 - dist_relevant_mse: 14.4227 - val_loss: 0.4213 - val_prob_loss: 0.3183 - val_dist_loss: 2.0607 - val_prob_kld: 0.0208 - val_dist_relevant_mae: 2.0593 - val_dist_relevant_mse: 14.8463\n",
            "Epoch 112/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3874 - prob_loss: 0.2950 - dist_loss: 1.8485 - prob_kld: 0.0061 - dist_relevant_mae: 1.8471 - dist_relevant_mse: 14.0927 - val_loss: 0.4061 - val_prob_loss: 0.3063 - val_dist_loss: 1.9952 - val_prob_kld: 0.0089 - val_dist_relevant_mae: 1.9937 - val_dist_relevant_mse: 14.8550\n",
            "Epoch 113/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3860 - prob_loss: 0.2944 - dist_loss: 1.8316 - prob_kld: 0.0054 - dist_relevant_mae: 1.8302 - dist_relevant_mse: 14.1576 - val_loss: 0.4064 - val_prob_loss: 0.3057 - val_dist_loss: 2.0127 - val_prob_kld: 0.0083 - val_dist_relevant_mae: 2.0113 - val_dist_relevant_mse: 14.9050\n",
            "Epoch 114/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3875 - prob_loss: 0.2965 - dist_loss: 1.8205 - prob_kld: 0.0059 - dist_relevant_mae: 1.8191 - dist_relevant_mse: 14.1682 - val_loss: 0.4075 - val_prob_loss: 0.3063 - val_dist_loss: 2.0240 - val_prob_kld: 0.0089 - val_dist_relevant_mae: 2.0226 - val_dist_relevant_mse: 15.3843\n",
            "Epoch 115/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3855 - prob_loss: 0.2955 - dist_loss: 1.7997 - prob_kld: 0.0055 - dist_relevant_mae: 1.7983 - dist_relevant_mse: 13.9581 - val_loss: 0.4053 - val_prob_loss: 0.3052 - val_dist_loss: 2.0019 - val_prob_kld: 0.0078 - val_dist_relevant_mae: 2.0005 - val_dist_relevant_mse: 14.4488\n",
            "Epoch 116/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.3892 - prob_loss: 0.2983 - dist_loss: 1.8178 - prob_kld: 0.0071 - dist_relevant_mae: 1.8164 - dist_relevant_mse: 14.6646 - val_loss: 0.4107 - val_prob_loss: 0.3106 - val_dist_loss: 2.0021 - val_prob_kld: 0.0132 - val_dist_relevant_mae: 2.0007 - val_dist_relevant_mse: 14.8130\n",
            "Epoch 117/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.3874 - prob_loss: 0.2960 - dist_loss: 1.8283 - prob_kld: 0.0054 - dist_relevant_mae: 1.8269 - dist_relevant_mse: 14.2219 - val_loss: 0.4045 - val_prob_loss: 0.3056 - val_dist_loss: 1.9775 - val_prob_kld: 0.0082 - val_dist_relevant_mae: 1.9761 - val_dist_relevant_mse: 14.5726\n",
            "Epoch 118/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3872 - prob_loss: 0.2958 - dist_loss: 1.8281 - prob_kld: 0.0059 - dist_relevant_mae: 1.8267 - dist_relevant_mse: 14.3168 - val_loss: 0.4048 - val_prob_loss: 0.3061 - val_dist_loss: 1.9743 - val_prob_kld: 0.0086 - val_dist_relevant_mae: 1.9729 - val_dist_relevant_mse: 14.6315\n",
            "Epoch 119/200\n",
            "100/100 [==============================] - 179s 2s/step - loss: 0.3864 - prob_loss: 0.2970 - dist_loss: 1.7889 - prob_kld: 0.0055 - dist_relevant_mae: 1.7875 - dist_relevant_mse: 13.5395 - val_loss: 0.4167 - val_prob_loss: 0.3120 - val_dist_loss: 2.0924 - val_prob_kld: 0.0146 - val_dist_relevant_mae: 2.0911 - val_dist_relevant_mse: 15.6523\n",
            "Epoch 120/200\n",
            "  2/100 [..............................] - ETA: 1:27 - loss: 0.3825 - prob_loss: 0.2925 - dist_loss: 1.8002 - prob_kld: 0.0084 - dist_relevant_mae: 1.7989 - dist_relevant_mse: 13.2862"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.200260). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 178s 2s/step - loss: 0.3890 - prob_loss: 0.2999 - dist_loss: 1.7816 - prob_kld: 0.0081 - dist_relevant_mae: 1.7802 - dist_relevant_mse: 14.2106 - val_loss: 0.4034 - val_prob_loss: 0.3056 - val_dist_loss: 1.9571 - val_prob_kld: 0.0081 - val_dist_relevant_mae: 1.9557 - val_dist_relevant_mse: 14.5332\n",
            "Epoch 121/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3841 - prob_loss: 0.2953 - dist_loss: 1.7762 - prob_kld: 0.0049 - dist_relevant_mae: 1.7748 - dist_relevant_mse: 13.8904 - val_loss: 0.4065 - val_prob_loss: 0.3054 - val_dist_loss: 2.0209 - val_prob_kld: 0.0080 - val_dist_relevant_mae: 2.0195 - val_dist_relevant_mse: 15.2672\n",
            "Epoch 122/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3823 - prob_loss: 0.2951 - dist_loss: 1.7437 - prob_kld: 0.0056 - dist_relevant_mae: 1.7423 - dist_relevant_mse: 13.5113 - val_loss: 0.4054 - val_prob_loss: 0.3052 - val_dist_loss: 2.0052 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 2.0038 - val_dist_relevant_mse: 14.7394\n",
            "Epoch 123/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3816 - prob_loss: 0.2923 - dist_loss: 1.7866 - prob_kld: 0.0051 - dist_relevant_mae: 1.7852 - dist_relevant_mse: 13.9544 - val_loss: 0.4045 - val_prob_loss: 0.3051 - val_dist_loss: 1.9871 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.9857 - val_dist_relevant_mse: 14.9435\n",
            "Epoch 124/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3882 - prob_loss: 0.2980 - dist_loss: 1.8050 - prob_kld: 0.0078 - dist_relevant_mae: 1.8036 - dist_relevant_mse: 14.2623 - val_loss: 0.4028 - val_prob_loss: 0.3049 - val_dist_loss: 1.9571 - val_prob_kld: 0.0075 - val_dist_relevant_mae: 1.9557 - val_dist_relevant_mse: 14.0106\n",
            "Epoch 125/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3817 - prob_loss: 0.2943 - dist_loss: 1.7473 - prob_kld: 0.0052 - dist_relevant_mae: 1.7459 - dist_relevant_mse: 13.5960 - val_loss: 0.4018 - val_prob_loss: 0.3056 - val_dist_loss: 1.9252 - val_prob_kld: 0.0081 - val_dist_relevant_mae: 1.9238 - val_dist_relevant_mse: 14.1162\n",
            "Epoch 126/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3829 - prob_loss: 0.2946 - dist_loss: 1.7667 - prob_kld: 0.0048 - dist_relevant_mae: 1.7653 - dist_relevant_mse: 13.5336 - val_loss: 0.4016 - val_prob_loss: 0.3041 - val_dist_loss: 1.9491 - val_prob_kld: 0.0067 - val_dist_relevant_mae: 1.9477 - val_dist_relevant_mse: 14.4255\n",
            "Epoch 127/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3865 - prob_loss: 0.2971 - dist_loss: 1.7892 - prob_kld: 0.0059 - dist_relevant_mae: 1.7878 - dist_relevant_mse: 13.7890 - val_loss: 0.4016 - val_prob_loss: 0.3050 - val_dist_loss: 1.9326 - val_prob_kld: 0.0076 - val_dist_relevant_mae: 1.9312 - val_dist_relevant_mse: 14.4242\n",
            "Epoch 128/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3870 - prob_loss: 0.3000 - dist_loss: 1.7397 - prob_kld: 0.0090 - dist_relevant_mae: 1.7383 - dist_relevant_mse: 13.3013 - val_loss: 0.4096 - val_prob_loss: 0.3087 - val_dist_loss: 2.0196 - val_prob_kld: 0.0112 - val_dist_relevant_mae: 2.0183 - val_dist_relevant_mse: 14.8803\n",
            "Epoch 129/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3841 - prob_loss: 0.2960 - dist_loss: 1.7628 - prob_kld: 0.0055 - dist_relevant_mae: 1.7614 - dist_relevant_mse: 13.6369 - val_loss: 0.4043 - val_prob_loss: 0.3051 - val_dist_loss: 1.9851 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.9837 - val_dist_relevant_mse: 14.4181\n",
            "Epoch 130/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3848 - prob_loss: 0.2980 - dist_loss: 1.7357 - prob_kld: 0.0080 - dist_relevant_mae: 1.7343 - dist_relevant_mse: 13.6383 - val_loss: 0.4123 - val_prob_loss: 0.3136 - val_dist_loss: 1.9745 - val_prob_kld: 0.0161 - val_dist_relevant_mae: 1.9731 - val_dist_relevant_mse: 14.7229\n",
            "Epoch 131/200\n",
            "  2/100 [..............................] - ETA: 1:26 - loss: 0.3781 - prob_loss: 0.2931 - dist_loss: 1.6990 - prob_kld: 0.0107 - dist_relevant_mae: 1.6976 - dist_relevant_mse: 11.9264"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.192509). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 177s 2s/step - loss: 0.3815 - prob_loss: 0.2971 - dist_loss: 1.6875 - prob_kld: 0.0075 - dist_relevant_mae: 1.6861 - dist_relevant_mse: 12.8988 - val_loss: 0.4059 - val_prob_loss: 0.3115 - val_dist_loss: 1.8873 - val_prob_kld: 0.0141 - val_dist_relevant_mae: 1.8859 - val_dist_relevant_mse: 13.8933\n",
            "Epoch 132/200\n",
            "  2/100 [..............................] - ETA: 1:05 - loss: 0.3715 - prob_loss: 0.2913 - dist_loss: 1.6041 - prob_kld: 0.0068 - dist_relevant_mae: 1.6027 - dist_relevant_mse: 10.7872"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.180913). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 177s 2s/step - loss: 0.3803 - prob_loss: 0.2950 - dist_loss: 1.7063 - prob_kld: 0.0052 - dist_relevant_mae: 1.7049 - dist_relevant_mse: 12.8978 - val_loss: 0.4139 - val_prob_loss: 0.3069 - val_dist_loss: 2.1412 - val_prob_kld: 0.0094 - val_dist_relevant_mae: 2.1399 - val_dist_relevant_mse: 16.0071\n",
            "Epoch 133/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3822 - prob_loss: 0.2941 - dist_loss: 1.7624 - prob_kld: 0.0046 - dist_relevant_mae: 1.7610 - dist_relevant_mse: 13.2192 - val_loss: 0.4061 - val_prob_loss: 0.3056 - val_dist_loss: 2.0099 - val_prob_kld: 0.0081 - val_dist_relevant_mae: 2.0086 - val_dist_relevant_mse: 14.9257\n",
            "Epoch 134/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3794 - prob_loss: 0.2932 - dist_loss: 1.7256 - prob_kld: 0.0047 - dist_relevant_mae: 1.7242 - dist_relevant_mse: 12.9690 - val_loss: 0.4005 - val_prob_loss: 0.3057 - val_dist_loss: 1.8952 - val_prob_kld: 0.0083 - val_dist_relevant_mae: 1.8937 - val_dist_relevant_mse: 13.3949\n",
            "Epoch 135/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3813 - prob_loss: 0.2952 - dist_loss: 1.7216 - prob_kld: 0.0047 - dist_relevant_mae: 1.7202 - dist_relevant_mse: 13.0642 - val_loss: 0.4025 - val_prob_loss: 0.3046 - val_dist_loss: 1.9586 - val_prob_kld: 0.0072 - val_dist_relevant_mae: 1.9572 - val_dist_relevant_mse: 14.4072\n",
            "Epoch 136/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3823 - prob_loss: 0.2952 - dist_loss: 1.7421 - prob_kld: 0.0047 - dist_relevant_mae: 1.7407 - dist_relevant_mse: 13.1008 - val_loss: 0.3987 - val_prob_loss: 0.3053 - val_dist_loss: 1.8683 - val_prob_kld: 0.0078 - val_dist_relevant_mae: 1.8669 - val_dist_relevant_mse: 13.5362\n",
            "Epoch 137/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3782 - prob_loss: 0.2944 - dist_loss: 1.6747 - prob_kld: 0.0050 - dist_relevant_mae: 1.6733 - dist_relevant_mse: 12.7621 - val_loss: 0.4025 - val_prob_loss: 0.3073 - val_dist_loss: 1.9041 - val_prob_kld: 0.0098 - val_dist_relevant_mae: 1.9027 - val_dist_relevant_mse: 14.1262\n",
            "Epoch 138/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3767 - prob_loss: 0.2939 - dist_loss: 1.6558 - prob_kld: 0.0044 - dist_relevant_mae: 1.6544 - dist_relevant_mse: 12.4397 - val_loss: 0.3990 - val_prob_loss: 0.3058 - val_dist_loss: 1.8639 - val_prob_kld: 0.0083 - val_dist_relevant_mae: 1.8625 - val_dist_relevant_mse: 13.7711\n",
            "Epoch 139/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3775 - prob_loss: 0.2950 - dist_loss: 1.6513 - prob_kld: 0.0047 - dist_relevant_mae: 1.6499 - dist_relevant_mse: 12.2461 - val_loss: 0.3976 - val_prob_loss: 0.3059 - val_dist_loss: 1.8341 - val_prob_kld: 0.0085 - val_dist_relevant_mae: 1.8327 - val_dist_relevant_mse: 13.3787\n",
            "Epoch 140/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3770 - prob_loss: 0.2946 - dist_loss: 1.6466 - prob_kld: 0.0050 - dist_relevant_mae: 1.6452 - dist_relevant_mse: 12.4020 - val_loss: 0.3998 - val_prob_loss: 0.3057 - val_dist_loss: 1.8828 - val_prob_kld: 0.0082 - val_dist_relevant_mae: 1.8814 - val_dist_relevant_mse: 13.8385\n",
            "Epoch 141/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3804 - prob_loss: 0.2972 - dist_loss: 1.6648 - prob_kld: 0.0069 - dist_relevant_mae: 1.6634 - dist_relevant_mse: 12.9143 - val_loss: 0.3997 - val_prob_loss: 0.3053 - val_dist_loss: 1.8874 - val_prob_kld: 0.0079 - val_dist_relevant_mae: 1.8860 - val_dist_relevant_mse: 14.0428\n",
            "Epoch 142/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3798 - prob_loss: 0.2969 - dist_loss: 1.6583 - prob_kld: 0.0060 - dist_relevant_mae: 1.6569 - dist_relevant_mse: 12.9771 - val_loss: 0.3976 - val_prob_loss: 0.3052 - val_dist_loss: 1.8488 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.8474 - val_dist_relevant_mse: 13.5172\n",
            "Epoch 143/200\n",
            "100/100 [==============================] - 170s 2s/step - loss: 0.3750 - prob_loss: 0.2944 - dist_loss: 1.6118 - prob_kld: 0.0045 - dist_relevant_mae: 1.6104 - dist_relevant_mse: 12.1054 - val_loss: 0.4013 - val_prob_loss: 0.3055 - val_dist_loss: 1.9164 - val_prob_kld: 0.0080 - val_dist_relevant_mae: 1.9150 - val_dist_relevant_mse: 14.2033\n",
            "Epoch 144/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3783 - prob_loss: 0.2951 - dist_loss: 1.6638 - prob_kld: 0.0048 - dist_relevant_mae: 1.6624 - dist_relevant_mse: 12.9767 - val_loss: 0.3979 - val_prob_loss: 0.3058 - val_dist_loss: 1.8409 - val_prob_kld: 0.0084 - val_dist_relevant_mae: 1.8395 - val_dist_relevant_mse: 13.5942\n",
            "Epoch 145/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3798 - prob_loss: 0.2960 - dist_loss: 1.6769 - prob_kld: 0.0048 - dist_relevant_mae: 1.6755 - dist_relevant_mse: 12.9097 - val_loss: 0.4002 - val_prob_loss: 0.3084 - val_dist_loss: 1.8357 - val_prob_kld: 0.0110 - val_dist_relevant_mae: 1.8343 - val_dist_relevant_mse: 13.4054\n",
            "Epoch 146/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3807 - prob_loss: 0.2974 - dist_loss: 1.6659 - prob_kld: 0.0084 - dist_relevant_mae: 1.6644 - dist_relevant_mse: 12.6502 - val_loss: 0.4128 - val_prob_loss: 0.3179 - val_dist_loss: 1.8971 - val_prob_kld: 0.0205 - val_dist_relevant_mae: 1.8957 - val_dist_relevant_mse: 13.9267\n",
            "Epoch 147/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3757 - prob_loss: 0.2945 - dist_loss: 1.6241 - prob_kld: 0.0051 - dist_relevant_mae: 1.6227 - dist_relevant_mse: 12.0890 - val_loss: 0.4010 - val_prob_loss: 0.3046 - val_dist_loss: 1.9265 - val_prob_kld: 0.0072 - val_dist_relevant_mae: 1.9251 - val_dist_relevant_mse: 14.2277\n",
            "Epoch 148/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3788 - prob_loss: 0.2944 - dist_loss: 1.6871 - prob_kld: 0.0049 - dist_relevant_mae: 1.6856 - dist_relevant_mse: 12.7234 - val_loss: 0.3971 - val_prob_loss: 0.3055 - val_dist_loss: 1.8326 - val_prob_kld: 0.0080 - val_dist_relevant_mae: 1.8311 - val_dist_relevant_mse: 13.1111\n",
            "Epoch 149/200\n",
            "100/100 [==============================] - 170s 2s/step - loss: 0.3775 - prob_loss: 0.2935 - dist_loss: 1.6801 - prob_kld: 0.0044 - dist_relevant_mae: 1.6787 - dist_relevant_mse: 12.9999 - val_loss: 0.3992 - val_prob_loss: 0.3067 - val_dist_loss: 1.8514 - val_prob_kld: 0.0092 - val_dist_relevant_mae: 1.8499 - val_dist_relevant_mse: 13.4914\n",
            "Epoch 150/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3754 - prob_loss: 0.2947 - dist_loss: 1.6135 - prob_kld: 0.0046 - dist_relevant_mae: 1.6121 - dist_relevant_mse: 12.5387 - val_loss: 0.4011 - val_prob_loss: 0.3076 - val_dist_loss: 1.8700 - val_prob_kld: 0.0102 - val_dist_relevant_mae: 1.8686 - val_dist_relevant_mse: 13.7012\n",
            "Epoch 151/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3756 - prob_loss: 0.2957 - dist_loss: 1.5988 - prob_kld: 0.0046 - dist_relevant_mae: 1.5974 - dist_relevant_mse: 12.0170 - val_loss: 0.3972 - val_prob_loss: 0.3055 - val_dist_loss: 1.8332 - val_prob_kld: 0.0081 - val_dist_relevant_mae: 1.8318 - val_dist_relevant_mse: 13.4228\n",
            "Epoch 152/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3800 - prob_loss: 0.2984 - dist_loss: 1.6325 - prob_kld: 0.0089 - dist_relevant_mae: 1.6311 - dist_relevant_mse: 12.1258 - val_loss: 0.3981 - val_prob_loss: 0.3073 - val_dist_loss: 1.8159 - val_prob_kld: 0.0098 - val_dist_relevant_mae: 1.8145 - val_dist_relevant_mse: 13.4954\n",
            "Epoch 153/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3751 - prob_loss: 0.2946 - dist_loss: 1.6090 - prob_kld: 0.0058 - dist_relevant_mae: 1.6076 - dist_relevant_mse: 12.1472 - val_loss: 0.3944 - val_prob_loss: 0.3056 - val_dist_loss: 1.7758 - val_prob_kld: 0.0081 - val_dist_relevant_mae: 1.7744 - val_dist_relevant_mse: 12.8537\n",
            "Epoch 154/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3730 - prob_loss: 0.2932 - dist_loss: 1.5963 - prob_kld: 0.0042 - dist_relevant_mae: 1.5949 - dist_relevant_mse: 12.0662 - val_loss: 0.3948 - val_prob_loss: 0.3055 - val_dist_loss: 1.7861 - val_prob_kld: 0.0081 - val_dist_relevant_mae: 1.7847 - val_dist_relevant_mse: 13.1141\n",
            "Epoch 155/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3740 - prob_loss: 0.2936 - dist_loss: 1.6078 - prob_kld: 0.0043 - dist_relevant_mae: 1.6064 - dist_relevant_mse: 12.1922 - val_loss: 0.4046 - val_prob_loss: 0.3052 - val_dist_loss: 1.9876 - val_prob_kld: 0.0078 - val_dist_relevant_mae: 1.9861 - val_dist_relevant_mse: 13.2613\n",
            "Epoch 156/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3801 - prob_loss: 0.2961 - dist_loss: 1.6798 - prob_kld: 0.0050 - dist_relevant_mae: 1.6784 - dist_relevant_mse: 12.5029 - val_loss: 0.3976 - val_prob_loss: 0.3059 - val_dist_loss: 1.8343 - val_prob_kld: 0.0085 - val_dist_relevant_mae: 1.8329 - val_dist_relevant_mse: 13.3709\n",
            "Epoch 157/200\n",
            "100/100 [==============================] - 177s 2s/step - loss: 0.3752 - prob_loss: 0.2956 - dist_loss: 1.5922 - prob_kld: 0.0045 - dist_relevant_mae: 1.5908 - dist_relevant_mse: 12.2974 - val_loss: 0.3973 - val_prob_loss: 0.3061 - val_dist_loss: 1.8230 - val_prob_kld: 0.0087 - val_dist_relevant_mae: 1.8216 - val_dist_relevant_mse: 13.2932\n",
            "Epoch 158/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3730 - prob_loss: 0.2938 - dist_loss: 1.5837 - prob_kld: 0.0043 - dist_relevant_mae: 1.5823 - dist_relevant_mse: 12.0556 - val_loss: 0.3992 - val_prob_loss: 0.3051 - val_dist_loss: 1.8824 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.8810 - val_dist_relevant_mse: 13.6953\n",
            "Epoch 159/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3758 - prob_loss: 0.2949 - dist_loss: 1.6173 - prob_kld: 0.0049 - dist_relevant_mae: 1.6159 - dist_relevant_mse: 12.2365 - val_loss: 0.3963 - val_prob_loss: 0.3057 - val_dist_loss: 1.8133 - val_prob_kld: 0.0082 - val_dist_relevant_mae: 1.8119 - val_dist_relevant_mse: 13.1367\n",
            "Epoch 160/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3746 - prob_loss: 0.2947 - dist_loss: 1.5984 - prob_kld: 0.0047 - dist_relevant_mae: 1.5970 - dist_relevant_mse: 12.1243 - val_loss: 0.3944 - val_prob_loss: 0.3055 - val_dist_loss: 1.7786 - val_prob_kld: 0.0080 - val_dist_relevant_mae: 1.7771 - val_dist_relevant_mse: 12.9466\n",
            "Epoch 161/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3773 - prob_loss: 0.2979 - dist_loss: 1.5891 - prob_kld: 0.0074 - dist_relevant_mae: 1.5877 - dist_relevant_mse: 11.7559 - val_loss: 0.3989 - val_prob_loss: 0.3058 - val_dist_loss: 1.8620 - val_prob_kld: 0.0084 - val_dist_relevant_mae: 1.8606 - val_dist_relevant_mse: 13.6874\n",
            "Epoch 162/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3749 - prob_loss: 0.2957 - dist_loss: 1.5829 - prob_kld: 0.0051 - dist_relevant_mae: 1.5815 - dist_relevant_mse: 11.8231 - val_loss: 0.4251 - val_prob_loss: 0.3352 - val_dist_loss: 1.7976 - val_prob_kld: 0.0378 - val_dist_relevant_mae: 1.7962 - val_dist_relevant_mse: 12.8580\n",
            "Epoch 163/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3740 - prob_loss: 0.2945 - dist_loss: 1.5912 - prob_kld: 0.0052 - dist_relevant_mae: 1.5898 - dist_relevant_mse: 11.5317 - val_loss: 0.3924 - val_prob_loss: 0.3048 - val_dist_loss: 1.7518 - val_prob_kld: 0.0074 - val_dist_relevant_mae: 1.7504 - val_dist_relevant_mse: 12.6032\n",
            "Epoch 164/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3756 - prob_loss: 0.2937 - dist_loss: 1.6378 - prob_kld: 0.0042 - dist_relevant_mae: 1.6364 - dist_relevant_mse: 11.9380 - val_loss: 0.3959 - val_prob_loss: 0.3057 - val_dist_loss: 1.8049 - val_prob_kld: 0.0082 - val_dist_relevant_mae: 1.8035 - val_dist_relevant_mse: 13.2880\n",
            "Epoch 165/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3736 - prob_loss: 0.2954 - dist_loss: 1.5651 - prob_kld: 0.0049 - dist_relevant_mae: 1.5637 - dist_relevant_mse: 11.9359 - val_loss: 0.4043 - val_prob_loss: 0.3089 - val_dist_loss: 1.9074 - val_prob_kld: 0.0115 - val_dist_relevant_mae: 1.9060 - val_dist_relevant_mse: 13.8993\n",
            "Epoch 166/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3742 - prob_loss: 0.2962 - dist_loss: 1.5596 - prob_kld: 0.0051 - dist_relevant_mae: 1.5582 - dist_relevant_mse: 11.8257 - val_loss: 0.3927 - val_prob_loss: 0.3053 - val_dist_loss: 1.7470 - val_prob_kld: 0.0079 - val_dist_relevant_mae: 1.7456 - val_dist_relevant_mse: 12.8133\n",
            "Epoch 167/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3753 - prob_loss: 0.2944 - dist_loss: 1.6190 - prob_kld: 0.0043 - dist_relevant_mae: 1.6176 - dist_relevant_mse: 12.5861 - val_loss: 0.4046 - val_prob_loss: 0.3062 - val_dist_loss: 1.9682 - val_prob_kld: 0.0087 - val_dist_relevant_mae: 1.9668 - val_dist_relevant_mse: 14.3845\n",
            "Epoch 168/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3749 - prob_loss: 0.2951 - dist_loss: 1.5968 - prob_kld: 0.0051 - dist_relevant_mae: 1.5954 - dist_relevant_mse: 11.9137 - val_loss: 0.3926 - val_prob_loss: 0.3052 - val_dist_loss: 1.7481 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.7467 - val_dist_relevant_mse: 12.6173\n",
            "Epoch 169/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3742 - prob_loss: 0.2969 - dist_loss: 1.5456 - prob_kld: 0.0062 - dist_relevant_mae: 1.5442 - dist_relevant_mse: 11.7974 - val_loss: 0.3987 - val_prob_loss: 0.3107 - val_dist_loss: 1.7596 - val_prob_kld: 0.0133 - val_dist_relevant_mae: 1.7582 - val_dist_relevant_mse: 12.9607\n",
            "Epoch 170/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3748 - prob_loss: 0.2978 - dist_loss: 1.5386 - prob_kld: 0.0068 - dist_relevant_mae: 1.5372 - dist_relevant_mse: 11.7776 - val_loss: 0.3985 - val_prob_loss: 0.3063 - val_dist_loss: 1.8437 - val_prob_kld: 0.0089 - val_dist_relevant_mae: 1.8423 - val_dist_relevant_mse: 13.5278\n",
            "Epoch 171/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3726 - prob_loss: 0.2964 - dist_loss: 1.5252 - prob_kld: 0.0042 - dist_relevant_mae: 1.5237 - dist_relevant_mse: 11.5823 - val_loss: 0.3955 - val_prob_loss: 0.3046 - val_dist_loss: 1.8172 - val_prob_kld: 0.0072 - val_dist_relevant_mae: 1.8158 - val_dist_relevant_mse: 13.3275\n",
            "Epoch 172/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3710 - prob_loss: 0.2946 - dist_loss: 1.5280 - prob_kld: 0.0039 - dist_relevant_mae: 1.5265 - dist_relevant_mse: 11.3826 - val_loss: 0.3919 - val_prob_loss: 0.3045 - val_dist_loss: 1.7473 - val_prob_kld: 0.0071 - val_dist_relevant_mae: 1.7459 - val_dist_relevant_mse: 12.8497\n",
            "Epoch 173/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3714 - prob_loss: 0.2941 - dist_loss: 1.5463 - prob_kld: 0.0044 - dist_relevant_mae: 1.5449 - dist_relevant_mse: 11.5416 - val_loss: 0.3932 - val_prob_loss: 0.3048 - val_dist_loss: 1.7685 - val_prob_kld: 0.0073 - val_dist_relevant_mae: 1.7671 - val_dist_relevant_mse: 12.9260\n",
            "Epoch 174/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3716 - prob_loss: 0.2945 - dist_loss: 1.5419 - prob_kld: 0.0044 - dist_relevant_mae: 1.5405 - dist_relevant_mse: 11.7720 - val_loss: 0.3917 - val_prob_loss: 0.3045 - val_dist_loss: 1.7427 - val_prob_kld: 0.0071 - val_dist_relevant_mae: 1.7413 - val_dist_relevant_mse: 12.7173\n",
            "Epoch 175/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3698 - prob_loss: 0.2943 - dist_loss: 1.5103 - prob_kld: 0.0041 - dist_relevant_mae: 1.5089 - dist_relevant_mse: 11.4020 - val_loss: 0.3978 - val_prob_loss: 0.3054 - val_dist_loss: 1.8492 - val_prob_kld: 0.0079 - val_dist_relevant_mae: 1.8478 - val_dist_relevant_mse: 13.5635\n",
            "Epoch 176/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3711 - prob_loss: 0.2948 - dist_loss: 1.5249 - prob_kld: 0.0043 - dist_relevant_mae: 1.5235 - dist_relevant_mse: 11.2557 - val_loss: 0.3901 - val_prob_loss: 0.3043 - val_dist_loss: 1.7150 - val_prob_kld: 0.0069 - val_dist_relevant_mae: 1.7136 - val_dist_relevant_mse: 12.2750\n",
            "Epoch 177/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3679 - prob_loss: 0.2936 - dist_loss: 1.4859 - prob_kld: 0.0044 - dist_relevant_mae: 1.4845 - dist_relevant_mse: 10.9184 - val_loss: 0.3912 - val_prob_loss: 0.3052 - val_dist_loss: 1.7192 - val_prob_kld: 0.0078 - val_dist_relevant_mae: 1.7178 - val_dist_relevant_mse: 12.5256\n",
            "Epoch 178/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3706 - prob_loss: 0.2947 - dist_loss: 1.5175 - prob_kld: 0.0041 - dist_relevant_mae: 1.5161 - dist_relevant_mse: 11.4697 - val_loss: 0.3926 - val_prob_loss: 0.3051 - val_dist_loss: 1.7487 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.7472 - val_dist_relevant_mse: 12.7898\n",
            "Epoch 179/200\n",
            "  2/100 [..............................] - ETA: 1:23 - loss: 0.3599 - prob_loss: 0.2877 - dist_loss: 1.4445 - prob_kld: 0.0045 - dist_relevant_mae: 1.4431 - dist_relevant_mse: 9.1436"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.203637). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 175s 2s/step - loss: 0.3723 - prob_loss: 0.2957 - dist_loss: 1.5307 - prob_kld: 0.0050 - dist_relevant_mae: 1.5293 - dist_relevant_mse: 11.5071 - val_loss: 0.3937 - val_prob_loss: 0.3052 - val_dist_loss: 1.7704 - val_prob_kld: 0.0078 - val_dist_relevant_mae: 1.7690 - val_dist_relevant_mse: 12.9190\n",
            "Epoch 180/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3703 - prob_loss: 0.2952 - dist_loss: 1.5028 - prob_kld: 0.0043 - dist_relevant_mae: 1.5014 - dist_relevant_mse: 11.0618 - val_loss: 0.3942 - val_prob_loss: 0.3086 - val_dist_loss: 1.7127 - val_prob_kld: 0.0112 - val_dist_relevant_mae: 1.7113 - val_dist_relevant_mse: 12.5467\n",
            "Epoch 181/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3689 - prob_loss: 0.2949 - dist_loss: 1.4813 - prob_kld: 0.0052 - dist_relevant_mae: 1.4799 - dist_relevant_mse: 11.0343 - val_loss: 0.3965 - val_prob_loss: 0.3079 - val_dist_loss: 1.7724 - val_prob_kld: 0.0105 - val_dist_relevant_mae: 1.7710 - val_dist_relevant_mse: 12.9428\n",
            "Epoch 182/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3698 - prob_loss: 0.2950 - dist_loss: 1.4953 - prob_kld: 0.0054 - dist_relevant_mae: 1.4938 - dist_relevant_mse: 10.9445 - val_loss: 0.3940 - val_prob_loss: 0.3066 - val_dist_loss: 1.7480 - val_prob_kld: 0.0091 - val_dist_relevant_mae: 1.7466 - val_dist_relevant_mse: 12.7702\n",
            "Epoch 183/200\n",
            "100/100 [==============================] - 176s 2s/step - loss: 0.3691 - prob_loss: 0.2936 - dist_loss: 1.5111 - prob_kld: 0.0046 - dist_relevant_mae: 1.5097 - dist_relevant_mse: 11.1209 - val_loss: 0.3914 - val_prob_loss: 0.3041 - val_dist_loss: 1.7467 - val_prob_kld: 0.0067 - val_dist_relevant_mae: 1.7453 - val_dist_relevant_mse: 12.2346\n",
            "Epoch 184/200\n",
            "100/100 [==============================] - 174s 2s/step - loss: 0.3731 - prob_loss: 0.2952 - dist_loss: 1.5592 - prob_kld: 0.0055 - dist_relevant_mae: 1.5577 - dist_relevant_mse: 11.3387 - val_loss: 0.4008 - val_prob_loss: 0.3050 - val_dist_loss: 1.9155 - val_prob_kld: 0.0076 - val_dist_relevant_mae: 1.9141 - val_dist_relevant_mse: 13.8818\n",
            "Epoch 185/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3738 - prob_loss: 0.2959 - dist_loss: 1.5575 - prob_kld: 0.0045 - dist_relevant_mae: 1.5560 - dist_relevant_mse: 12.1095 - val_loss: 0.3918 - val_prob_loss: 0.3064 - val_dist_loss: 1.7074 - val_prob_kld: 0.0090 - val_dist_relevant_mae: 1.7059 - val_dist_relevant_mse: 12.4026\n",
            "Epoch 186/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3700 - prob_loss: 0.2931 - dist_loss: 1.5384 - prob_kld: 0.0041 - dist_relevant_mae: 1.5369 - dist_relevant_mse: 11.3863 - val_loss: 0.3966 - val_prob_loss: 0.3054 - val_dist_loss: 1.8236 - val_prob_kld: 0.0080 - val_dist_relevant_mae: 1.8222 - val_dist_relevant_mse: 13.1496\n",
            "Epoch 187/200\n",
            "100/100 [==============================] - 170s 2s/step - loss: 0.3748 - prob_loss: 0.2971 - dist_loss: 1.5543 - prob_kld: 0.0058 - dist_relevant_mae: 1.5529 - dist_relevant_mse: 11.3931 - val_loss: 0.3917 - val_prob_loss: 0.3049 - val_dist_loss: 1.7355 - val_prob_kld: 0.0075 - val_dist_relevant_mae: 1.7341 - val_dist_relevant_mse: 12.6081\n",
            "Epoch 188/200\n",
            "100/100 [==============================] - 168s 2s/step - loss: 0.3693 - prob_loss: 0.2932 - dist_loss: 1.5209 - prob_kld: 0.0043 - dist_relevant_mae: 1.5195 - dist_relevant_mse: 11.4645 - val_loss: 0.3890 - val_prob_loss: 0.3045 - val_dist_loss: 1.6910 - val_prob_kld: 0.0071 - val_dist_relevant_mae: 1.6896 - val_dist_relevant_mse: 12.1455\n",
            "Epoch 189/200\n",
            "100/100 [==============================] - 169s 2s/step - loss: 0.3676 - prob_loss: 0.2934 - dist_loss: 1.4823 - prob_kld: 0.0038 - dist_relevant_mae: 1.4809 - dist_relevant_mse: 10.9794 - val_loss: 0.3904 - val_prob_loss: 0.3052 - val_dist_loss: 1.7053 - val_prob_kld: 0.0077 - val_dist_relevant_mae: 1.7039 - val_dist_relevant_mse: 12.0787\n",
            "Epoch 190/200\n",
            "100/100 [==============================] - 170s 2s/step - loss: 0.3701 - prob_loss: 0.2952 - dist_loss: 1.4982 - prob_kld: 0.0062 - dist_relevant_mae: 1.4967 - dist_relevant_mse: 11.4057 - val_loss: 0.3887 - val_prob_loss: 0.3042 - val_dist_loss: 1.6909 - val_prob_kld: 0.0067 - val_dist_relevant_mae: 1.6895 - val_dist_relevant_mse: 12.0924\n",
            "Epoch 191/200\n",
            "100/100 [==============================] - 169s 2s/step - loss: 0.3663 - prob_loss: 0.2926 - dist_loss: 1.4741 - prob_kld: 0.0038 - dist_relevant_mae: 1.4727 - dist_relevant_mse: 11.0405 - val_loss: 0.3881 - val_prob_loss: 0.3045 - val_dist_loss: 1.6723 - val_prob_kld: 0.0071 - val_dist_relevant_mae: 1.6708 - val_dist_relevant_mse: 12.1319\n",
            "Epoch 192/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3656 - prob_loss: 0.2933 - dist_loss: 1.4455 - prob_kld: 0.0038 - dist_relevant_mae: 1.4440 - dist_relevant_mse: 10.3653 - val_loss: 0.3878 - val_prob_loss: 0.3045 - val_dist_loss: 1.6660 - val_prob_kld: 0.0071 - val_dist_relevant_mae: 1.6646 - val_dist_relevant_mse: 12.1555\n",
            "Epoch 193/200\n",
            "100/100 [==============================] - 173s 2s/step - loss: 0.3658 - prob_loss: 0.2931 - dist_loss: 1.4539 - prob_kld: 0.0047 - dist_relevant_mae: 1.4525 - dist_relevant_mse: 10.5457 - val_loss: 0.3888 - val_prob_loss: 0.3047 - val_dist_loss: 1.6817 - val_prob_kld: 0.0073 - val_dist_relevant_mae: 1.6803 - val_dist_relevant_mse: 12.2910\n",
            "Epoch 194/200\n",
            "100/100 [==============================] - 171s 2s/step - loss: 0.3678 - prob_loss: 0.2952 - dist_loss: 1.4515 - prob_kld: 0.0041 - dist_relevant_mae: 1.4501 - dist_relevant_mse: 10.6599 - val_loss: 0.3899 - val_prob_loss: 0.3060 - val_dist_loss: 1.6771 - val_prob_kld: 0.0086 - val_dist_relevant_mae: 1.6757 - val_dist_relevant_mse: 11.8432\n",
            "Epoch 195/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3675 - prob_loss: 0.2937 - dist_loss: 1.4751 - prob_kld: 0.0040 - dist_relevant_mae: 1.4737 - dist_relevant_mse: 10.9391 - val_loss: 0.3896 - val_prob_loss: 0.3040 - val_dist_loss: 1.7111 - val_prob_kld: 0.0066 - val_dist_relevant_mae: 1.7096 - val_dist_relevant_mse: 11.9850\n",
            "Epoch 196/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3697 - prob_loss: 0.2952 - dist_loss: 1.4910 - prob_kld: 0.0053 - dist_relevant_mae: 1.4896 - dist_relevant_mse: 11.4819 - val_loss: 0.3899 - val_prob_loss: 0.3047 - val_dist_loss: 1.7049 - val_prob_kld: 0.0073 - val_dist_relevant_mae: 1.7035 - val_dist_relevant_mse: 12.2578\n",
            "Epoch 197/200\n",
            "100/100 [==============================] - 172s 2s/step - loss: 0.3666 - prob_loss: 0.2930 - dist_loss: 1.4716 - prob_kld: 0.0039 - dist_relevant_mae: 1.4702 - dist_relevant_mse: 11.2416 - val_loss: 0.3867 - val_prob_loss: 0.3040 - val_dist_loss: 1.6548 - val_prob_kld: 0.0065 - val_dist_relevant_mae: 1.6533 - val_dist_relevant_mse: 12.0855\n",
            "Epoch 198/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3693 - prob_loss: 0.2945 - dist_loss: 1.4956 - prob_kld: 0.0046 - dist_relevant_mae: 1.4942 - dist_relevant_mse: 11.1780 - val_loss: 0.3978 - val_prob_loss: 0.3127 - val_dist_loss: 1.7018 - val_prob_kld: 0.0153 - val_dist_relevant_mae: 1.7004 - val_dist_relevant_mse: 12.4087\n",
            "Epoch 199/200\n",
            "  2/100 [..............................] - ETA: 1:28 - loss: 0.3752 - prob_loss: 0.3070 - dist_loss: 1.3630 - prob_kld: 0.0075 - dist_relevant_mae: 1.3616 - dist_relevant_mse: 10.1958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.187090). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 175s 2s/step - loss: 0.3695 - prob_loss: 0.2952 - dist_loss: 1.4876 - prob_kld: 0.0048 - dist_relevant_mae: 1.4862 - dist_relevant_mse: 10.7554 - val_loss: 0.3868 - val_prob_loss: 0.3035 - val_dist_loss: 1.6653 - val_prob_kld: 0.0061 - val_dist_relevant_mae: 1.6638 - val_dist_relevant_mse: 11.8497\n",
            "Epoch 200/200\n",
            "100/100 [==============================] - 175s 2s/step - loss: 0.3677 - prob_loss: 0.2947 - dist_loss: 1.4612 - prob_kld: 0.0036 - dist_relevant_mae: 1.4598 - dist_relevant_mse: 11.0554 - val_loss: 0.3881 - val_prob_loss: 0.3039 - val_dist_loss: 1.6848 - val_prob_kld: 0.0065 - val_dist_relevant_mae: 1.6834 - val_dist_relevant_mse: 12.2113\n",
            "\n",
            "Loading network weights from 'weights_last.h5'.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "NMS threshold = 0.3:  75%|███████▌  | 15/20 [00:09<00:03,  1.61it/s, 0.668 -> 0.839]\n",
            "NMS threshold = 0.4:  75%|███████▌  | 15/20 [00:08<00:02,  1.87it/s, 0.668 -> 0.839]\n",
            "NMS threshold = 0.5:  75%|███████▌  | 15/20 [00:07<00:02,  1.89it/s, 0.668 -> 0.839]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using optimized values: prob_thresh=0.665958, nms_thresh=0.3.\n",
            "Saving to 'thresholds.json'.\n",
            "Training UNET model\n",
            "number of training images:\t 173\n",
            "number of validation images:\t 19\n",
            "image size (2D):\t\t (256, 256)\n",
            "axes:\t\t\t\t SYXC\n",
            "channels in / out:\t\t 1 / 1\n",
            "Config(axes='YXC', n_channel_in=1, n_channel_out=1, n_dim=2, probabilistic=False, train_batch_size=1, train_checkpoint='weights_best.h5', train_checkpoint_epoch='weights_now.h5', train_checkpoint_last='weights_last.h5', train_epochs=200, train_learning_rate=0.0001, train_loss='mae', train_reduce_lr={'patience': 5, 'factor': 0.5}, train_steps_per_epoch=400, train_tensorboard=True, unet_input_shape=(None, None, 1), unet_kern_size=3, unet_last_activation='linear', unet_n_depth=4, unet_n_first=48, unet_residual=True)\n",
            "Epoch 1/200\n",
            "400/400 [==============================] - 10s 24ms/step - loss: 0.1830 - mse: 0.1120 - mae: 0.1830 - val_loss: 0.1036 - val_mse: 0.0695 - val_mae: 0.1036\n",
            "Epoch 2/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0920 - mse: 0.0629 - mae: 0.0920 - val_loss: 0.0831 - val_mse: 0.0583 - val_mae: 0.0831\n",
            "Epoch 3/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0680 - mse: 0.0458 - mae: 0.0680 - val_loss: 0.0613 - val_mse: 0.0442 - val_mae: 0.0613\n",
            "Epoch 4/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0519 - mse: 0.0334 - mae: 0.0519 - val_loss: 0.0537 - val_mse: 0.0358 - val_mae: 0.0537\n",
            "Epoch 5/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0419 - mse: 0.0249 - mae: 0.0419 - val_loss: 0.0461 - val_mse: 0.0296 - val_mae: 0.0461\n",
            "Epoch 6/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0358 - mse: 0.0203 - mae: 0.0358 - val_loss: 0.0541 - val_mse: 0.0274 - val_mae: 0.0541\n",
            "Epoch 7/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0313 - mse: 0.0172 - mae: 0.0313 - val_loss: 0.0403 - val_mse: 0.0253 - val_mae: 0.0403\n",
            "Epoch 8/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0271 - mse: 0.0149 - mae: 0.0271 - val_loss: 0.0369 - val_mse: 0.0257 - val_mae: 0.0369\n",
            "Epoch 9/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0264 - mse: 0.0137 - mae: 0.0264 - val_loss: 0.0318 - val_mse: 0.0219 - val_mae: 0.0318\n",
            "Epoch 10/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0237 - mse: 0.0120 - mae: 0.0237 - val_loss: 0.0396 - val_mse: 0.0263 - val_mae: 0.0396\n",
            "Epoch 11/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0209 - mse: 0.0103 - mae: 0.0209 - val_loss: 0.0367 - val_mse: 0.0226 - val_mae: 0.0367\n",
            "Epoch 12/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0203 - mse: 0.0102 - mae: 0.0203 - val_loss: 0.0298 - val_mse: 0.0204 - val_mae: 0.0298\n",
            "Epoch 13/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0195 - mse: 0.0101 - mae: 0.0195 - val_loss: 0.0297 - val_mse: 0.0198 - val_mae: 0.0297\n",
            "Epoch 14/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0183 - mse: 0.0089 - mae: 0.0183 - val_loss: 0.0300 - val_mse: 0.0199 - val_mae: 0.0300\n",
            "Epoch 15/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0175 - mse: 0.0090 - mae: 0.0175 - val_loss: 0.0343 - val_mse: 0.0233 - val_mae: 0.0343\n",
            "Epoch 16/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0163 - mse: 0.0083 - mae: 0.0163 - val_loss: 0.0290 - val_mse: 0.0194 - val_mae: 0.0290\n",
            "Epoch 17/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0162 - mse: 0.0082 - mae: 0.0162 - val_loss: 0.0249 - val_mse: 0.0190 - val_mae: 0.0249\n",
            "Epoch 18/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0152 - mse: 0.0074 - mae: 0.0152 - val_loss: 0.0257 - val_mse: 0.0188 - val_mae: 0.0257\n",
            "Epoch 19/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0146 - mse: 0.0076 - mae: 0.0146 - val_loss: 0.0253 - val_mse: 0.0192 - val_mae: 0.0253\n",
            "Epoch 20/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0147 - mse: 0.0074 - mae: 0.0147 - val_loss: 0.0253 - val_mse: 0.0184 - val_mae: 0.0253\n",
            "Epoch 21/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0136 - mse: 0.0065 - mae: 0.0136 - val_loss: 0.0233 - val_mse: 0.0182 - val_mae: 0.0233\n",
            "Epoch 22/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0135 - mse: 0.0063 - mae: 0.0135 - val_loss: 0.0259 - val_mse: 0.0187 - val_mae: 0.0259\n",
            "Epoch 23/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0130 - mse: 0.0065 - mae: 0.0130 - val_loss: 0.0224 - val_mse: 0.0175 - val_mae: 0.0224\n",
            "Epoch 24/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0125 - mse: 0.0062 - mae: 0.0125 - val_loss: 0.0227 - val_mse: 0.0180 - val_mae: 0.0227\n",
            "Epoch 25/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0126 - mse: 0.0064 - mae: 0.0126 - val_loss: 0.0233 - val_mse: 0.0181 - val_mae: 0.0233\n",
            "Epoch 26/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0121 - mse: 0.0063 - mae: 0.0121 - val_loss: 0.0233 - val_mse: 0.0175 - val_mae: 0.0233\n",
            "Epoch 27/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0109 - mse: 0.0054 - mae: 0.0109 - val_loss: 0.0226 - val_mse: 0.0173 - val_mae: 0.0226\n",
            "Epoch 28/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0114 - mse: 0.0057 - mae: 0.0114 - val_loss: 0.0269 - val_mse: 0.0184 - val_mae: 0.0269\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "Epoch 29/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0073 - mse: 0.0036 - mae: 0.0073 - val_loss: 0.0204 - val_mse: 0.0164 - val_mae: 0.0204\n",
            "Epoch 30/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0065 - mse: 0.0029 - mae: 0.0065 - val_loss: 0.0205 - val_mse: 0.0164 - val_mae: 0.0205\n",
            "Epoch 31/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0065 - mse: 0.0029 - mae: 0.0065 - val_loss: 0.0200 - val_mse: 0.0162 - val_mae: 0.0200\n",
            "Epoch 32/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0062 - mse: 0.0027 - mae: 0.0062 - val_loss: 0.0207 - val_mse: 0.0164 - val_mae: 0.0207\n",
            "Epoch 33/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0068 - mse: 0.0026 - mae: 0.0068 - val_loss: 0.0199 - val_mse: 0.0162 - val_mae: 0.0199\n",
            "Epoch 34/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0065 - mse: 0.0028 - mae: 0.0065 - val_loss: 0.0215 - val_mse: 0.0166 - val_mae: 0.0215\n",
            "Epoch 35/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0063 - mse: 0.0028 - mae: 0.0063 - val_loss: 0.0215 - val_mse: 0.0166 - val_mae: 0.0215\n",
            "Epoch 36/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0068 - mse: 0.0029 - mae: 0.0068 - val_loss: 0.0207 - val_mse: 0.0165 - val_mae: 0.0207\n",
            "\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "Epoch 37/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0046 - mse: 0.0020 - mae: 0.0046 - val_loss: 0.0196 - val_mse: 0.0162 - val_mae: 0.0196\n",
            "Epoch 38/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0041 - mse: 0.0016 - mae: 0.0041 - val_loss: 0.0192 - val_mse: 0.0161 - val_mae: 0.0192\n",
            "Epoch 39/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0040 - mse: 0.0016 - mae: 0.0040 - val_loss: 0.0194 - val_mse: 0.0163 - val_mae: 0.0194\n",
            "Epoch 40/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0041 - mse: 0.0015 - mae: 0.0041 - val_loss: 0.0196 - val_mse: 0.0161 - val_mae: 0.0196\n",
            "Epoch 41/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0041 - mse: 0.0016 - mae: 0.0041 - val_loss: 0.0198 - val_mse: 0.0163 - val_mae: 0.0198\n",
            "Epoch 42/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0040 - mse: 0.0015 - mae: 0.0040 - val_loss: 0.0193 - val_mse: 0.0164 - val_mae: 0.0193\n",
            "Epoch 43/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0040 - mse: 0.0016 - mae: 0.0040 - val_loss: 0.0198 - val_mse: 0.0162 - val_mae: 0.0198\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "Epoch 44/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0033 - mse: 0.0013 - mae: 0.0033 - val_loss: 0.0189 - val_mse: 0.0162 - val_mae: 0.0189\n",
            "Epoch 45/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0031 - mse: 0.0011 - mae: 0.0031 - val_loss: 0.0189 - val_mse: 0.0162 - val_mae: 0.0189\n",
            "Epoch 46/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0030 - mse: 0.0011 - mae: 0.0030 - val_loss: 0.0190 - val_mse: 0.0164 - val_mae: 0.0190\n",
            "Epoch 47/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0031 - mse: 0.0011 - mae: 0.0031 - val_loss: 0.0190 - val_mse: 0.0163 - val_mae: 0.0190\n",
            "Epoch 48/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0030 - mse: 0.0011 - mae: 0.0030 - val_loss: 0.0190 - val_mse: 0.0163 - val_mae: 0.0190\n",
            "Epoch 49/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0030 - mse: 0.0011 - mae: 0.0030 - val_loss: 0.0189 - val_mse: 0.0163 - val_mae: 0.0189\n",
            "\n",
            "Epoch 00049: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "Epoch 50/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0028 - mse: 0.0010 - mae: 0.0028 - val_loss: 0.0189 - val_mse: 0.0163 - val_mae: 0.0189\n",
            "Epoch 51/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0026 - mse: 9.5975e-04 - mae: 0.0026 - val_loss: 0.0188 - val_mse: 0.0163 - val_mae: 0.0188\n",
            "Epoch 52/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0026 - mse: 9.6691e-04 - mae: 0.0026 - val_loss: 0.0189 - val_mse: 0.0163 - val_mae: 0.0189\n",
            "Epoch 53/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0026 - mse: 9.6485e-04 - mae: 0.0026 - val_loss: 0.0188 - val_mse: 0.0163 - val_mae: 0.0188\n",
            "Epoch 54/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0026 - mse: 9.6624e-04 - mae: 0.0026 - val_loss: 0.0188 - val_mse: 0.0163 - val_mae: 0.0188\n",
            "Epoch 55/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0026 - mse: 9.6151e-04 - mae: 0.0026 - val_loss: 0.0188 - val_mse: 0.0164 - val_mae: 0.0188\n",
            "Epoch 56/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0026 - mse: 9.5702e-04 - mae: 0.0026 - val_loss: 0.0188 - val_mse: 0.0163 - val_mae: 0.0188\n",
            "Epoch 57/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0025 - mse: 9.3778e-04 - mae: 0.0025 - val_loss: 0.0188 - val_mse: 0.0163 - val_mae: 0.0188\n",
            "Epoch 58/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0026 - mse: 9.5060e-04 - mae: 0.0026 - val_loss: 0.0188 - val_mse: 0.0163 - val_mae: 0.0188\n",
            "Epoch 59/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0025 - mse: 9.2227e-04 - mae: 0.0025 - val_loss: 0.0188 - val_mse: 0.0164 - val_mae: 0.0188\n",
            "\n",
            "Epoch 00059: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
            "Epoch 60/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0024 - mse: 8.9626e-04 - mae: 0.0024 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 61/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0024 - mse: 9.0269e-04 - mae: 0.0024 - val_loss: 0.0187 - val_mse: 0.0163 - val_mae: 0.0187\n",
            "Epoch 62/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0024 - mse: 8.8418e-04 - mae: 0.0024 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 63/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0024 - mse: 8.9106e-04 - mae: 0.0024 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 64/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0023 - mse: 8.7237e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
            "Epoch 65/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0023 - mse: 8.8638e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 66/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0023 - mse: 8.5817e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 67/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0023 - mse: 8.7671e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 68/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0023 - mse: 8.5225e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 69/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0023 - mse: 8.6993e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
            "Epoch 70/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0023 - mse: 8.6610e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 71/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0023 - mse: 8.5409e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 72/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0023 - mse: 8.7097e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 73/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4441e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 74/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0023 - mse: 8.6618e-04 - mae: 0.0023 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
            "Epoch 75/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5000e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 76/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5200e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 77/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5454e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 78/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5298e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 79/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5530e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 80/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4518e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 81/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4733e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 82/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5147e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 83/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5046e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 84/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4013e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
            "Epoch 85/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5062e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 86/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5149e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 87/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3539e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 88/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4608e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 89/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4387e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00089: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
            "Epoch 90/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5686e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 91/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3119e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 92/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5126e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 93/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4235e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 94/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4195e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00094: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
            "Epoch 95/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5053e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 96/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3895e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 97/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4332e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 98/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4426e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 99/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4264e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00099: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
            "Epoch 100/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3059e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 101/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.6637e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 102/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2935e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 103/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5164e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 104/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4013e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
            "Epoch 105/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4760e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 106/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3225e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 107/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4081e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 108/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5202e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 109/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3281e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-09.\n",
            "Epoch 110/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5112e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 111/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3122e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 112/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4484e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 113/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5963e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 114/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2906e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00114: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-09.\n",
            "Epoch 115/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3579e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 116/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4839e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 117/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3427e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 118/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5267e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 119/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3961e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00119: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-09.\n",
            "Epoch 120/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4523e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 121/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5006e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 122/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3576e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 123/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2884e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 124/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4910e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00124: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-10.\n",
            "Epoch 125/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4914e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 126/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3561e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 127/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4729e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 128/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4394e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 129/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5018e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00129: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-10.\n",
            "Epoch 130/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3672e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 131/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3768e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 132/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4453e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 133/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3952e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 134/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4067e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00134: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-10.\n",
            "Epoch 135/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4318e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 136/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5436e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 137/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3478e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 138/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4454e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 139/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4620e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00139: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-11.\n",
            "Epoch 140/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3450e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 141/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4188e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 142/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3601e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 143/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4436e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 144/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4665e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00144: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-11.\n",
            "Epoch 145/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4294e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 146/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4818e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 147/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3382e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 148/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5386e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 149/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2965e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00149: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-11.\n",
            "Epoch 150/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4169e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 151/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5074e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 152/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.2993e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 153/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5562e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 154/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3111e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-11.\n",
            "Epoch 155/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5638e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 156/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3008e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 157/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4762e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 158/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4941e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 159/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4708e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00159: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-12.\n",
            "Epoch 160/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3644e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 161/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3868e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 162/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2434e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 163/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5947e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 164/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3744e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-12.\n",
            "Epoch 165/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3957e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 166/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4708e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 167/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3889e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 168/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4403e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 169/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5202e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00169: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-12.\n",
            "Epoch 170/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3995e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 171/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3539e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 172/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4801e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 173/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4122e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 174/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4067e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00174: ReduceLROnPlateau reducing learning rate to 7.450580408706331e-13.\n",
            "Epoch 175/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3566e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 176/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.5314e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 177/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4834e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 178/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4069e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 179/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4132e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00179: ReduceLROnPlateau reducing learning rate to 3.7252902043531655e-13.\n",
            "Epoch 180/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4031e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 181/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.3421e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 182/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4945e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 183/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3816e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 184/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5380e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00184: ReduceLROnPlateau reducing learning rate to 1.8626451021765827e-13.\n",
            "Epoch 185/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2705e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 186/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4906e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 187/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4234e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 188/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4149e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 189/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.4268e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00189: ReduceLROnPlateau reducing learning rate to 9.313225510882914e-14.\n",
            "Epoch 190/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4082e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 191/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5871e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 192/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.2943e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 193/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5153e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 194/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.2420e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00194: ReduceLROnPlateau reducing learning rate to 4.656612755441457e-14.\n",
            "Epoch 195/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4312e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 196/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3539e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 197/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4849e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 198/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.5433e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "Epoch 199/200\n",
            "400/400 [==============================] - 8s 19ms/step - loss: 0.0022 - mse: 8.4140e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Epoch 00199: ReduceLROnPlateau reducing learning rate to 2.3283063777207284e-14.\n",
            "Epoch 200/200\n",
            "400/400 [==============================] - 8s 20ms/step - loss: 0.0022 - mse: 8.3540e-04 - mae: 0.0022 - val_loss: 0.0187 - val_mse: 0.0164 - val_mae: 0.0187\n",
            "\n",
            "Loading network weights from 'weights_best.h5'.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rGWrjXn4yn1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}